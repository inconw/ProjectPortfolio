{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69edb12d-6f2d-45d5-bb2f-f9c89520e6e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BERT\n",
    "## Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "### Ingrid J. Conway\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88442705-128b-48c2-a27c-2b6988323fc3",
   "metadata": {},
   "source": [
    "\n",
    "* https://keras.io/api/keras_nlp/\n",
    "\n",
    "    * https://keras.io/api/keras_nlp/models/\n",
    "\n",
    "    * https://keras.io/api/keras_nlp/models/bert/bert_classifier/#bertclassifier-class\n",
    "\n",
    "    * https://keras.io/examples/nlp/text_extraction_with_bert/\n",
    "\n",
    "    * https://keras.io/examples/nlp/semantic_similarity_with_bert/\n",
    "\n",
    "    * https://keras.io/examples/nlp/masked_language_modeling/\n",
    "\n",
    "    * https://keras.io/examples/nlp/pretraining_BERT/\n",
    "\n",
    "\n",
    "* https://www.tensorflow.org/text/tutorials/classify_text_with_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7503ddd-a394-431d-8ed1-dd7790e9c262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from tensorflow_addons.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47a0e7e-f8f2-4c64-8f65-5ed9e18fc270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "#from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d21c90ad-a21b-4419-8b3d-6fd0aa05a986",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d432e56-e212-459f-b331-5414d7adac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'aclImdb', 'aclImdb_v1.tar.gz', 'BERT + TWITTER.ipynb', 'BERT social media sentiment analysis.ipynb', 'bert_data', 'bert_en_uncased_preprocess_3.tar.gz', 'Bert_Sentiment_june.ipynb', 'Corona_NLP_test.csv', 'Corona_NLP_train.csv', 'Depression _ Anxiety Facebook page Comments Text.xlsx', 'Sentimental_ML_7k.ipynb', 'Sentiment_Analysis_FaceBook_7k.csv', 'SMSCollection.csv', 'Spam Ham SMS BERT.ipynb', 'Untitled1.ipynb']\n"
     ]
    }
   ],
   "source": [
    "#os.getcwd()\n",
    "print(os.listdir(os.getcwd()))\n",
    "os.chdir(\"C:\\\\Users\\\\ingri\\\\OneDrive\\\\SynergisticPython\\\\BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5325b391-ac29-43dd-bdae-0fdff6501b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2df9d07-ac77-4c96-9b3e-0e36eded25eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734a7d0-ec68-45ab-823f-65f74b241276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2065d2c-7ff1-4785-880e-a1aaf7b73d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7ec395-a71b-4438-8528-c1bfc30da6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef9998-b843-4458-963b-3b7cd105cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e4ec1-1cfd-4355-9e29-61a919a56f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b8c8a-1940-47d5-8ef9-d56fa7acfede",
   "metadata": {},
   "source": [
    "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
    "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d074adb-f455-4f06-aa27-6feb6a951c6c",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fcdcdb-022a-4bd3-b0e3-6cbcd312733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ca113-23d7-4594-a65c-e7180bd25d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea106d2a-89db-4ef3-8a86-6b7ec7ea56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = ['this is such an amazing movie!']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d89f2-c46e-4b4f-a2e9-f3409455a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee8f88-322c-4456-925d-52c562a75a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67afa41-7865-4e2f-9b33-bcf49cd8d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3beb013-41d1-494a-a85c-007412bc7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe1f6d-3df1-41aa-ac9d-f3a955450194",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df1ecb-92f4-42ea-8350-53af1a9b169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820970e-3668-4767-b2dc-50c77dca855f",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5fc24-5104-4ba3-909e-521c4bb57715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install tf-models-official\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fcced1-0d62-4639-a987-b575814be3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from official.nlp import optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c403d5-0dc4-4726-a258-1740c58da95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49799f7-8ef2-45b0-8461-a8b983439f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8988f8f-0bbe-4ac8-abb7-d5b98c685491",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27abfec1-9ca1-46d7-8893-02524ab549c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e423fc71-034e-436e-9e50-174cfb308165",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dcd579-f2f7-4c46-a99b-08abd5fed01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c69aa4-063f-47e1-a428-4696002e73bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot the accuracy and loss over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0de699e-cb02-4d62-ac43-55ba1a44d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46ec72-6172-4b77-a727-ccb148071512",
   "metadata": {},
   "source": [
    "### Export for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc8623-f2e2-480a-b163-2d3fce8c8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'imdb'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0107d-bb99-4585-af7e-801a932137cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d8b6d-04b0-4a06-879a-d3642d9a3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_my_examples(inputs, results):\n",
    "  result_for_printing = \\\n",
    "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
    "                         for i in range(len(inputs))]\n",
    "  print(*result_for_printing, sep='\\n')\n",
    "  print()\n",
    "\n",
    "\n",
    "examples = [\n",
    "    'this is such an amazing movie!',  # this is the same sentence tried earlier\n",
    "    'The movie was great!',\n",
    "    'The movie was meh.',\n",
    "    'The movie was okish.',\n",
    "    'The movie was terrible...'\n",
    "]\n",
    "\n",
    "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
    "\n",
    "print('Results from the saved model:')\n",
    "print_my_examples(examples, reloaded_results)\n",
    "print('Results from the model in memory:')\n",
    "print_my_examples(examples, original_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
