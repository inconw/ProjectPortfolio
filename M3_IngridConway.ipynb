{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inconw/ProjectPortfolio/blob/main/M3_IngridConway.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9-WPn4bLNY6"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NQHx5AsLRlg"
      },
      "source": [
        "# Milestone 3\n",
        "Ingrid Conway\n",
        "November 30, 2020\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This semester's milestone uses data collected for the semiconductor industry to the disposable diaper industry. Disposable diapers make up a vast industry. In 1996 diaper sales exceeded $4 billion in the U.S. alone ('Disposable Diaper'). With the advances in computing power in recent decades, the potential gains from well done data analytics are immense. \n",
        "\n",
        "The disposable diaper industry has continued to evolve since its inception in the early-1970s. Today's diapers are highly functional and efficient for households compared to traditional clothe diapers. Contemporary diapers include advanced features like special sizing and coloring for specific ages or different genders ('Disposable Diaper'). Above all, the single most important property of a diaper is its ability to absorb and retain moisture. Today's most advanced diapers absorb 15 times their weight in liquid.\n",
        "\n",
        "While the vast majority of diapers function perfectly, there is always a small portion with some defect. The aim of this project is to construct AI models for the diaper manufacturer to predict the occurance of a faulty diaper.\n",
        "\n",
        "\n",
        "\n",
        "\"Disposable Diaper.\" madehow.com. Retrieved Sept. 2020, from http://www.madehow.com/Volume-3/Disposable-Diaper.html#google_vignette\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYu9G0uxLX_c"
      },
      "source": [
        "# First, we import the necessary libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn import svm, metrics, datasets\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RdNoF6UMzlX",
        "outputId": "555d0348-ed39-4d33-d151-a0ab3f5aa527"
      },
      "source": [
        "# enable GPU for neural network training speedup\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "print(device_name)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k95MGNVtN9GO"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "FKTsBqWIM-9v",
        "outputId": "354e72f2-2d49-4b06-fc87-df7ba4bafedf"
      },
      "source": [
        "# When reading in file, must force it to not read a header row\n",
        "# The csv file contains no header\n",
        "label = pd.read_csv(\"secom_labels.data\", header=None)\n",
        "print(f'label csv file shape (rows, columns): {label.shape}')\n",
        "label.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label csv file shape (rows, columns): (1567, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1 \"19/07/2008 11:55:00\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1 \"19/07/2008 12:32:00\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1 \"19/07/2008 13:17:00\"</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          0\n",
              "0  -1 \"19/07/2008 11:55:00\"\n",
              "1  -1 \"19/07/2008 12:32:00\"\n",
              "2   1 \"19/07/2008 13:17:00\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "kC_51W6VOW_y",
        "outputId": "a1dc34b4-d2cb-4dfd-e5b9-e7ad3e1805ec"
      },
      "source": [
        "# split on the quotation mark, not on the spaces\n",
        "# Splitting on spaces splits the datetime, which we don't want\n",
        "label = label[0].str.split('\"', expand = True)\n",
        "label.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>19/07/2008 11:55:00</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>19/07/2008 12:32:00</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>19/07/2008 13:17:00</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0                    1 2\n",
              "0  -1   19/07/2008 11:55:00  \n",
              "1  -1   19/07/2008 12:32:00  \n",
              "2   1   19/07/2008 13:17:00  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buAAPc9COFhN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "2529e20d-338c-4823-ab57-788ddca8e7d9"
      },
      "source": [
        "# Drop the column labelled 2, because that column is empty\n",
        "label.drop([2], axis='columns', inplace=True)\n",
        "# relabel columns 1 and 2\n",
        "label.columns = ['passFail', 'datetime']\n",
        "label.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passFail</th>\n",
              "      <th>datetime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>19/07/2008 11:55:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>19/07/2008 12:32:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>19/07/2008 13:17:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  passFail             datetime\n",
              "0      -1   19/07/2008 11:55:00\n",
              "1      -1   19/07/2008 12:32:00\n",
              "2       1   19/07/2008 13:17:00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "qRMm4TbEtbdR",
        "outputId": "64ac23c2-bcdd-42dd-e03e-0d139526ea31"
      },
      "source": [
        "# convert datetime string into datetime object\n",
        "from dateutil.parser import parse\n",
        "label['datetime'] = label['datetime'].apply(parse)\n",
        "print(label.dtypes)\n",
        "label.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "passFail            object\n",
            "datetime    datetime64[ns]\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passFail</th>\n",
              "      <th>datetime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>2008-07-19 11:55:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>2008-07-19 12:32:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2008-07-19 13:17:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  passFail            datetime\n",
              "0      -1  2008-07-19 11:55:00\n",
              "1      -1  2008-07-19 12:32:00\n",
              "2       1  2008-07-19 13:17:00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "V--q0b7kuJ39",
        "outputId": "b79cd671-abd6-46a5-b0b9-ee76a501f400"
      },
      "source": [
        "# convert passFail to number, and convert -1 to zero, so only have labels 0 and 1\n",
        "label['passFail'] = label['passFail'].astype(int) \n",
        "label['passFail'] = label['passFail'].map({0:0, -1:0, 1:1}) # changed -1 to a zero; 1 stays a 1; 0 stays 0\n",
        "print(label.info())\n",
        "print(f'\\nCount of passFail labels:\\n{label[\"passFail\"].value_counts()}\\n')\n",
        "label.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1567 entries, 0 to 1566\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype         \n",
            "---  ------    --------------  -----         \n",
            " 0   passFail  1567 non-null   int64         \n",
            " 1   datetime  1567 non-null   datetime64[ns]\n",
            "dtypes: datetime64[ns](1), int64(1)\n",
            "memory usage: 24.6 KB\n",
            "None\n",
            "\n",
            "Count of passFail labels:\n",
            "0    1463\n",
            "1     104\n",
            "Name: passFail, dtype: int64\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passFail</th>\n",
              "      <th>datetime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2008-07-19 11:55:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2008-07-19 12:32:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2008-07-19 13:17:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   passFail            datetime\n",
              "0         0 2008-07-19 11:55:00\n",
              "1         0 2008-07-19 12:32:00\n",
              "2         1 2008-07-19 13:17:00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urCruUVSBb3V"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "_5zatxSUOdBS",
        "outputId": "c4be4fdc-82b5-44cf-b05b-b896c23cb90f"
      },
      "source": [
        "# Reading in sensor reading data\n",
        "# When reading in file, must force it to not read a header row\n",
        "# The csv file contains no header\n",
        "# Also, make the delimiter=\" \", because the rows are separated by a space instead \n",
        "# of the default columns.  \n",
        "secom = pd.read_csv(\"secom.data\", header=None, delimiter=\" \" )\n",
        "print(f'secom csv file shape (rows, columns): {secom.shape}')\n",
        "secom.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "secom csv file shape (rows, columns): (1567, 590)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>550</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "      <th>561</th>\n",
              "      <th>562</th>\n",
              "      <th>563</th>\n",
              "      <th>564</th>\n",
              "      <th>565</th>\n",
              "      <th>566</th>\n",
              "      <th>567</th>\n",
              "      <th>568</th>\n",
              "      <th>569</th>\n",
              "      <th>570</th>\n",
              "      <th>571</th>\n",
              "      <th>572</th>\n",
              "      <th>573</th>\n",
              "      <th>574</th>\n",
              "      <th>575</th>\n",
              "      <th>576</th>\n",
              "      <th>577</th>\n",
              "      <th>578</th>\n",
              "      <th>579</th>\n",
              "      <th>580</th>\n",
              "      <th>581</th>\n",
              "      <th>582</th>\n",
              "      <th>583</th>\n",
              "      <th>584</th>\n",
              "      <th>585</th>\n",
              "      <th>586</th>\n",
              "      <th>587</th>\n",
              "      <th>588</th>\n",
              "      <th>589</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3030.93</td>\n",
              "      <td>2564.00</td>\n",
              "      <td>2187.7333</td>\n",
              "      <td>1411.1265</td>\n",
              "      <td>1.3602</td>\n",
              "      <td>100.0</td>\n",
              "      <td>97.6133</td>\n",
              "      <td>0.1242</td>\n",
              "      <td>1.5005</td>\n",
              "      <td>0.0162</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>0.9455</td>\n",
              "      <td>202.4396</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.9558</td>\n",
              "      <td>414.8710</td>\n",
              "      <td>10.0433</td>\n",
              "      <td>0.9680</td>\n",
              "      <td>192.3963</td>\n",
              "      <td>12.5190</td>\n",
              "      <td>1.4026</td>\n",
              "      <td>-5419.00</td>\n",
              "      <td>2916.50</td>\n",
              "      <td>-4043.75</td>\n",
              "      <td>751.00</td>\n",
              "      <td>0.8955</td>\n",
              "      <td>1.7730</td>\n",
              "      <td>3.0490</td>\n",
              "      <td>64.2333</td>\n",
              "      <td>2.0222</td>\n",
              "      <td>0.1632</td>\n",
              "      <td>3.5191</td>\n",
              "      <td>83.3971</td>\n",
              "      <td>9.5126</td>\n",
              "      <td>50.6170</td>\n",
              "      <td>64.2588</td>\n",
              "      <td>49.3830</td>\n",
              "      <td>66.3141</td>\n",
              "      <td>86.9555</td>\n",
              "      <td>117.5132</td>\n",
              "      <td>...</td>\n",
              "      <td>12.93</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.1827</td>\n",
              "      <td>5.7349</td>\n",
              "      <td>0.3363</td>\n",
              "      <td>39.8842</td>\n",
              "      <td>3.2687</td>\n",
              "      <td>1.0297</td>\n",
              "      <td>1.0344</td>\n",
              "      <td>0.4385</td>\n",
              "      <td>0.1039</td>\n",
              "      <td>42.3877</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>533.8500</td>\n",
              "      <td>2.1113</td>\n",
              "      <td>8.95</td>\n",
              "      <td>0.3157</td>\n",
              "      <td>3.0624</td>\n",
              "      <td>0.1026</td>\n",
              "      <td>1.6765</td>\n",
              "      <td>14.9509</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.5005</td>\n",
              "      <td>0.0118</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>2.3630</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3095.78</td>\n",
              "      <td>2465.14</td>\n",
              "      <td>2230.4222</td>\n",
              "      <td>1463.6606</td>\n",
              "      <td>0.8294</td>\n",
              "      <td>100.0</td>\n",
              "      <td>102.3433</td>\n",
              "      <td>0.1247</td>\n",
              "      <td>1.4966</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>-0.0148</td>\n",
              "      <td>0.9627</td>\n",
              "      <td>200.5470</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.1548</td>\n",
              "      <td>414.7347</td>\n",
              "      <td>9.2599</td>\n",
              "      <td>0.9701</td>\n",
              "      <td>191.2872</td>\n",
              "      <td>12.4608</td>\n",
              "      <td>1.3825</td>\n",
              "      <td>-5441.50</td>\n",
              "      <td>2604.25</td>\n",
              "      <td>-3498.75</td>\n",
              "      <td>-1640.25</td>\n",
              "      <td>1.2973</td>\n",
              "      <td>2.0143</td>\n",
              "      <td>7.3900</td>\n",
              "      <td>68.4222</td>\n",
              "      <td>2.2667</td>\n",
              "      <td>0.2102</td>\n",
              "      <td>3.4171</td>\n",
              "      <td>84.9052</td>\n",
              "      <td>9.7997</td>\n",
              "      <td>50.6596</td>\n",
              "      <td>64.2828</td>\n",
              "      <td>49.3404</td>\n",
              "      <td>64.9193</td>\n",
              "      <td>87.5241</td>\n",
              "      <td>118.1188</td>\n",
              "      <td>...</td>\n",
              "      <td>16.00</td>\n",
              "      <td>1.33</td>\n",
              "      <td>0.2829</td>\n",
              "      <td>7.1196</td>\n",
              "      <td>0.4989</td>\n",
              "      <td>53.1836</td>\n",
              "      <td>3.9139</td>\n",
              "      <td>1.7819</td>\n",
              "      <td>0.9634</td>\n",
              "      <td>0.1745</td>\n",
              "      <td>0.0375</td>\n",
              "      <td>18.1087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>535.0164</td>\n",
              "      <td>2.4335</td>\n",
              "      <td>5.92</td>\n",
              "      <td>0.2653</td>\n",
              "      <td>2.0111</td>\n",
              "      <td>0.0772</td>\n",
              "      <td>1.1065</td>\n",
              "      <td>10.9003</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>208.2045</td>\n",
              "      <td>0.5019</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>4.4447</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>208.2045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2932.61</td>\n",
              "      <td>2559.94</td>\n",
              "      <td>2186.4111</td>\n",
              "      <td>1698.0172</td>\n",
              "      <td>1.5102</td>\n",
              "      <td>100.0</td>\n",
              "      <td>95.4878</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>1.4436</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.9615</td>\n",
              "      <td>202.0179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.5157</td>\n",
              "      <td>416.7075</td>\n",
              "      <td>9.3144</td>\n",
              "      <td>0.9674</td>\n",
              "      <td>192.7035</td>\n",
              "      <td>12.5404</td>\n",
              "      <td>1.4123</td>\n",
              "      <td>-5447.75</td>\n",
              "      <td>2701.75</td>\n",
              "      <td>-4047.00</td>\n",
              "      <td>-1916.50</td>\n",
              "      <td>1.3122</td>\n",
              "      <td>2.0295</td>\n",
              "      <td>7.5788</td>\n",
              "      <td>67.1333</td>\n",
              "      <td>2.3333</td>\n",
              "      <td>0.1734</td>\n",
              "      <td>3.5986</td>\n",
              "      <td>84.7569</td>\n",
              "      <td>8.6590</td>\n",
              "      <td>50.1530</td>\n",
              "      <td>64.1114</td>\n",
              "      <td>49.8470</td>\n",
              "      <td>65.8389</td>\n",
              "      <td>84.7327</td>\n",
              "      <td>118.6128</td>\n",
              "      <td>...</td>\n",
              "      <td>16.16</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.0857</td>\n",
              "      <td>7.1619</td>\n",
              "      <td>0.3752</td>\n",
              "      <td>23.0713</td>\n",
              "      <td>3.9306</td>\n",
              "      <td>1.1386</td>\n",
              "      <td>1.5021</td>\n",
              "      <td>0.3718</td>\n",
              "      <td>0.1233</td>\n",
              "      <td>24.7524</td>\n",
              "      <td>267.064</td>\n",
              "      <td>0.9032</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.6219</td>\n",
              "      <td>0.4122</td>\n",
              "      <td>0.2562</td>\n",
              "      <td>0.4119</td>\n",
              "      <td>68.8489</td>\n",
              "      <td>535.0245</td>\n",
              "      <td>2.0293</td>\n",
              "      <td>11.21</td>\n",
              "      <td>0.1882</td>\n",
              "      <td>4.0923</td>\n",
              "      <td>0.0640</td>\n",
              "      <td>2.0952</td>\n",
              "      <td>9.2721</td>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.0484</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>82.8602</td>\n",
              "      <td>0.4958</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>3.1745</td>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.0484</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>82.8602</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows Ã— 590 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0        1          2          3    ...     586     587     588       589\n",
              "0  3030.93  2564.00  2187.7333  1411.1265  ...     NaN     NaN     NaN       NaN\n",
              "1  3095.78  2465.14  2230.4222  1463.6606  ...  0.0096  0.0201  0.0060  208.2045\n",
              "2  2932.61  2559.94  2186.4111  1698.0172  ...  0.0584  0.0484  0.0148   82.8602\n",
              "\n",
              "[3 rows x 590 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuYtmjorOg2Q"
      },
      "source": [
        "# DO not need to do this str.split because read in file above with delimiter=\" \"\n",
        "#secom.columns = ['sensorReading']\n",
        "#secom = secom.sensorReading.str.split(expand = True)\n",
        "#print(f'secom csv file shape (rows, columns): {secom.shape}')\n",
        "#secom.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "iHdwHunvOoLn",
        "outputId": "6b16a7a0-bbdf-4463-ae4a-7f86d324281e"
      },
      "source": [
        "#Merging label and secom\n",
        "#secomMerged = label.append(secom)\n",
        "secomMerged = pd.concat([label, secom], axis = 1)\n",
        "print(f'merged dataframe shape (rows, columns): {secomMerged.shape}')\n",
        "secomMerged.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "merged dataframe shape (rows, columns): (1567, 592)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passFail</th>\n",
              "      <th>datetime</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>550</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "      <th>561</th>\n",
              "      <th>562</th>\n",
              "      <th>563</th>\n",
              "      <th>564</th>\n",
              "      <th>565</th>\n",
              "      <th>566</th>\n",
              "      <th>567</th>\n",
              "      <th>568</th>\n",
              "      <th>569</th>\n",
              "      <th>570</th>\n",
              "      <th>571</th>\n",
              "      <th>572</th>\n",
              "      <th>573</th>\n",
              "      <th>574</th>\n",
              "      <th>575</th>\n",
              "      <th>576</th>\n",
              "      <th>577</th>\n",
              "      <th>578</th>\n",
              "      <th>579</th>\n",
              "      <th>580</th>\n",
              "      <th>581</th>\n",
              "      <th>582</th>\n",
              "      <th>583</th>\n",
              "      <th>584</th>\n",
              "      <th>585</th>\n",
              "      <th>586</th>\n",
              "      <th>587</th>\n",
              "      <th>588</th>\n",
              "      <th>589</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2008-07-19 11:55:00</td>\n",
              "      <td>3030.93</td>\n",
              "      <td>2564.00</td>\n",
              "      <td>2187.7333</td>\n",
              "      <td>1411.1265</td>\n",
              "      <td>1.3602</td>\n",
              "      <td>100.0</td>\n",
              "      <td>97.6133</td>\n",
              "      <td>0.1242</td>\n",
              "      <td>1.5005</td>\n",
              "      <td>0.0162</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>0.9455</td>\n",
              "      <td>202.4396</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.9558</td>\n",
              "      <td>414.8710</td>\n",
              "      <td>10.0433</td>\n",
              "      <td>0.9680</td>\n",
              "      <td>192.3963</td>\n",
              "      <td>12.5190</td>\n",
              "      <td>1.4026</td>\n",
              "      <td>-5419.00</td>\n",
              "      <td>2916.50</td>\n",
              "      <td>-4043.75</td>\n",
              "      <td>751.00</td>\n",
              "      <td>0.8955</td>\n",
              "      <td>1.7730</td>\n",
              "      <td>3.0490</td>\n",
              "      <td>64.2333</td>\n",
              "      <td>2.0222</td>\n",
              "      <td>0.1632</td>\n",
              "      <td>3.5191</td>\n",
              "      <td>83.3971</td>\n",
              "      <td>9.5126</td>\n",
              "      <td>50.6170</td>\n",
              "      <td>64.2588</td>\n",
              "      <td>49.3830</td>\n",
              "      <td>66.3141</td>\n",
              "      <td>...</td>\n",
              "      <td>12.93</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.1827</td>\n",
              "      <td>5.7349</td>\n",
              "      <td>0.3363</td>\n",
              "      <td>39.8842</td>\n",
              "      <td>3.2687</td>\n",
              "      <td>1.0297</td>\n",
              "      <td>1.0344</td>\n",
              "      <td>0.4385</td>\n",
              "      <td>0.1039</td>\n",
              "      <td>42.3877</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>533.8500</td>\n",
              "      <td>2.1113</td>\n",
              "      <td>8.95</td>\n",
              "      <td>0.3157</td>\n",
              "      <td>3.0624</td>\n",
              "      <td>0.1026</td>\n",
              "      <td>1.6765</td>\n",
              "      <td>14.9509</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.5005</td>\n",
              "      <td>0.0118</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>2.3630</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2008-07-19 12:32:00</td>\n",
              "      <td>3095.78</td>\n",
              "      <td>2465.14</td>\n",
              "      <td>2230.4222</td>\n",
              "      <td>1463.6606</td>\n",
              "      <td>0.8294</td>\n",
              "      <td>100.0</td>\n",
              "      <td>102.3433</td>\n",
              "      <td>0.1247</td>\n",
              "      <td>1.4966</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>-0.0148</td>\n",
              "      <td>0.9627</td>\n",
              "      <td>200.5470</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.1548</td>\n",
              "      <td>414.7347</td>\n",
              "      <td>9.2599</td>\n",
              "      <td>0.9701</td>\n",
              "      <td>191.2872</td>\n",
              "      <td>12.4608</td>\n",
              "      <td>1.3825</td>\n",
              "      <td>-5441.50</td>\n",
              "      <td>2604.25</td>\n",
              "      <td>-3498.75</td>\n",
              "      <td>-1640.25</td>\n",
              "      <td>1.2973</td>\n",
              "      <td>2.0143</td>\n",
              "      <td>7.3900</td>\n",
              "      <td>68.4222</td>\n",
              "      <td>2.2667</td>\n",
              "      <td>0.2102</td>\n",
              "      <td>3.4171</td>\n",
              "      <td>84.9052</td>\n",
              "      <td>9.7997</td>\n",
              "      <td>50.6596</td>\n",
              "      <td>64.2828</td>\n",
              "      <td>49.3404</td>\n",
              "      <td>64.9193</td>\n",
              "      <td>...</td>\n",
              "      <td>16.00</td>\n",
              "      <td>1.33</td>\n",
              "      <td>0.2829</td>\n",
              "      <td>7.1196</td>\n",
              "      <td>0.4989</td>\n",
              "      <td>53.1836</td>\n",
              "      <td>3.9139</td>\n",
              "      <td>1.7819</td>\n",
              "      <td>0.9634</td>\n",
              "      <td>0.1745</td>\n",
              "      <td>0.0375</td>\n",
              "      <td>18.1087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>535.0164</td>\n",
              "      <td>2.4335</td>\n",
              "      <td>5.92</td>\n",
              "      <td>0.2653</td>\n",
              "      <td>2.0111</td>\n",
              "      <td>0.0772</td>\n",
              "      <td>1.1065</td>\n",
              "      <td>10.9003</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>208.2045</td>\n",
              "      <td>0.5019</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>4.4447</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>208.2045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2008-07-19 13:17:00</td>\n",
              "      <td>2932.61</td>\n",
              "      <td>2559.94</td>\n",
              "      <td>2186.4111</td>\n",
              "      <td>1698.0172</td>\n",
              "      <td>1.5102</td>\n",
              "      <td>100.0</td>\n",
              "      <td>95.4878</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>1.4436</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.9615</td>\n",
              "      <td>202.0179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.5157</td>\n",
              "      <td>416.7075</td>\n",
              "      <td>9.3144</td>\n",
              "      <td>0.9674</td>\n",
              "      <td>192.7035</td>\n",
              "      <td>12.5404</td>\n",
              "      <td>1.4123</td>\n",
              "      <td>-5447.75</td>\n",
              "      <td>2701.75</td>\n",
              "      <td>-4047.00</td>\n",
              "      <td>-1916.50</td>\n",
              "      <td>1.3122</td>\n",
              "      <td>2.0295</td>\n",
              "      <td>7.5788</td>\n",
              "      <td>67.1333</td>\n",
              "      <td>2.3333</td>\n",
              "      <td>0.1734</td>\n",
              "      <td>3.5986</td>\n",
              "      <td>84.7569</td>\n",
              "      <td>8.6590</td>\n",
              "      <td>50.1530</td>\n",
              "      <td>64.1114</td>\n",
              "      <td>49.8470</td>\n",
              "      <td>65.8389</td>\n",
              "      <td>...</td>\n",
              "      <td>16.16</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.0857</td>\n",
              "      <td>7.1619</td>\n",
              "      <td>0.3752</td>\n",
              "      <td>23.0713</td>\n",
              "      <td>3.9306</td>\n",
              "      <td>1.1386</td>\n",
              "      <td>1.5021</td>\n",
              "      <td>0.3718</td>\n",
              "      <td>0.1233</td>\n",
              "      <td>24.7524</td>\n",
              "      <td>267.064</td>\n",
              "      <td>0.9032</td>\n",
              "      <td>1.10</td>\n",
              "      <td>0.6219</td>\n",
              "      <td>0.4122</td>\n",
              "      <td>0.2562</td>\n",
              "      <td>0.4119</td>\n",
              "      <td>68.8489</td>\n",
              "      <td>535.0245</td>\n",
              "      <td>2.0293</td>\n",
              "      <td>11.21</td>\n",
              "      <td>0.1882</td>\n",
              "      <td>4.0923</td>\n",
              "      <td>0.0640</td>\n",
              "      <td>2.0952</td>\n",
              "      <td>9.2721</td>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.0484</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>82.8602</td>\n",
              "      <td>0.4958</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>3.1745</td>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.0484</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>82.8602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2008-07-19 14:43:00</td>\n",
              "      <td>2988.72</td>\n",
              "      <td>2479.90</td>\n",
              "      <td>2199.0333</td>\n",
              "      <td>909.7926</td>\n",
              "      <td>1.3204</td>\n",
              "      <td>100.0</td>\n",
              "      <td>104.2367</td>\n",
              "      <td>0.1217</td>\n",
              "      <td>1.4882</td>\n",
              "      <td>-0.0124</td>\n",
              "      <td>-0.0033</td>\n",
              "      <td>0.9629</td>\n",
              "      <td>201.8482</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.6052</td>\n",
              "      <td>422.2894</td>\n",
              "      <td>9.6924</td>\n",
              "      <td>0.9687</td>\n",
              "      <td>192.1557</td>\n",
              "      <td>12.4782</td>\n",
              "      <td>1.4011</td>\n",
              "      <td>-5468.25</td>\n",
              "      <td>2648.25</td>\n",
              "      <td>-4515.00</td>\n",
              "      <td>-1657.25</td>\n",
              "      <td>1.3137</td>\n",
              "      <td>2.0038</td>\n",
              "      <td>7.3145</td>\n",
              "      <td>62.9333</td>\n",
              "      <td>2.6444</td>\n",
              "      <td>0.2071</td>\n",
              "      <td>3.3813</td>\n",
              "      <td>84.9105</td>\n",
              "      <td>8.6789</td>\n",
              "      <td>50.5100</td>\n",
              "      <td>64.1125</td>\n",
              "      <td>49.4900</td>\n",
              "      <td>65.1951</td>\n",
              "      <td>...</td>\n",
              "      <td>131.68</td>\n",
              "      <td>39.33</td>\n",
              "      <td>0.6812</td>\n",
              "      <td>56.9303</td>\n",
              "      <td>17.4781</td>\n",
              "      <td>161.4081</td>\n",
              "      <td>35.3198</td>\n",
              "      <td>54.2917</td>\n",
              "      <td>1.1613</td>\n",
              "      <td>0.7288</td>\n",
              "      <td>0.2710</td>\n",
              "      <td>62.7572</td>\n",
              "      <td>268.228</td>\n",
              "      <td>0.6511</td>\n",
              "      <td>7.32</td>\n",
              "      <td>0.1630</td>\n",
              "      <td>3.5611</td>\n",
              "      <td>0.0670</td>\n",
              "      <td>2.7290</td>\n",
              "      <td>25.0363</td>\n",
              "      <td>530.5682</td>\n",
              "      <td>2.0253</td>\n",
              "      <td>9.33</td>\n",
              "      <td>0.1738</td>\n",
              "      <td>2.8971</td>\n",
              "      <td>0.0525</td>\n",
              "      <td>1.7585</td>\n",
              "      <td>8.5831</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>73.8432</td>\n",
              "      <td>0.4990</td>\n",
              "      <td>0.0103</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>2.0544</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>73.8432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2008-07-19 15:22:00</td>\n",
              "      <td>3032.24</td>\n",
              "      <td>2502.87</td>\n",
              "      <td>2233.3667</td>\n",
              "      <td>1326.5200</td>\n",
              "      <td>1.5334</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.3967</td>\n",
              "      <td>0.1235</td>\n",
              "      <td>1.5031</td>\n",
              "      <td>-0.0031</td>\n",
              "      <td>-0.0072</td>\n",
              "      <td>0.9569</td>\n",
              "      <td>201.9424</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.5661</td>\n",
              "      <td>420.5925</td>\n",
              "      <td>10.3387</td>\n",
              "      <td>0.9735</td>\n",
              "      <td>191.6037</td>\n",
              "      <td>12.4735</td>\n",
              "      <td>1.3888</td>\n",
              "      <td>-5476.25</td>\n",
              "      <td>2635.25</td>\n",
              "      <td>-3987.50</td>\n",
              "      <td>117.00</td>\n",
              "      <td>1.2887</td>\n",
              "      <td>1.9912</td>\n",
              "      <td>7.2748</td>\n",
              "      <td>62.8333</td>\n",
              "      <td>3.1556</td>\n",
              "      <td>0.2696</td>\n",
              "      <td>3.2728</td>\n",
              "      <td>86.3269</td>\n",
              "      <td>8.7677</td>\n",
              "      <td>50.2480</td>\n",
              "      <td>64.1511</td>\n",
              "      <td>49.7520</td>\n",
              "      <td>66.1542</td>\n",
              "      <td>...</td>\n",
              "      <td>19.63</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.4287</td>\n",
              "      <td>9.7608</td>\n",
              "      <td>0.8311</td>\n",
              "      <td>70.9706</td>\n",
              "      <td>4.9086</td>\n",
              "      <td>2.5014</td>\n",
              "      <td>0.9778</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.0461</td>\n",
              "      <td>22.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>532.0155</td>\n",
              "      <td>2.0275</td>\n",
              "      <td>8.83</td>\n",
              "      <td>0.2224</td>\n",
              "      <td>3.1776</td>\n",
              "      <td>0.0706</td>\n",
              "      <td>1.6597</td>\n",
              "      <td>10.9698</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.4800</td>\n",
              "      <td>0.4766</td>\n",
              "      <td>0.1045</td>\n",
              "      <td>99.3032</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>73.8432</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 592 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   passFail            datetime        0  ...     587     588       589\n",
              "0         0 2008-07-19 11:55:00  3030.93  ...     NaN     NaN       NaN\n",
              "1         0 2008-07-19 12:32:00  3095.78  ...  0.0201  0.0060  208.2045\n",
              "2         1 2008-07-19 13:17:00  2932.61  ...  0.0484  0.0148   82.8602\n",
              "3         0 2008-07-19 14:43:00  2988.72  ...  0.0149  0.0044   73.8432\n",
              "4         0 2008-07-19 15:22:00  3032.24  ...  0.0149  0.0044   73.8432\n",
              "\n",
              "[5 rows x 592 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYDp7ct6nZOY",
        "outputId": "f182119f-880b-4fd0-d09c-f0fd4ebb4dc5"
      },
      "source": [
        "# row 0, column 578 is NaN\n",
        "# what is this?  a string 'NaN', or the float value NaN?\n",
        "# because I read in the secom.data file with delimiter=\" \", pd.read_csv correctly\n",
        "# read in the values as floating point numbers, and also recognized the 'NaN' as \n",
        "# not a number float type.  Here are examples of the raw data and the data types\n",
        "print(f'value at row 0, column 0:       {secomMerged[0][0]}')\n",
        "print(f'data type at row 0, column 0:   {type(secomMerged[0][0])}')\n",
        "print(f'value at row 0, column 578:     {secomMerged[578][0]}')\n",
        "print(f'data type at row 0, column 578: {type(secomMerged[578][0])}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "value at row 0, column 0:       3030.93\n",
            "data type at row 0, column 0:   <class 'numpy.float64'>\n",
            "value at row 0, column 578:     nan\n",
            "data type at row 0, column 578: <class 'numpy.float64'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "eJobulSaOokH",
        "outputId": "0670f732-01d6-472b-b851-aa9d126bf602"
      },
      "source": [
        "secomMerged.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passFail</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>550</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "      <th>561</th>\n",
              "      <th>562</th>\n",
              "      <th>563</th>\n",
              "      <th>564</th>\n",
              "      <th>565</th>\n",
              "      <th>566</th>\n",
              "      <th>567</th>\n",
              "      <th>568</th>\n",
              "      <th>569</th>\n",
              "      <th>570</th>\n",
              "      <th>571</th>\n",
              "      <th>572</th>\n",
              "      <th>573</th>\n",
              "      <th>574</th>\n",
              "      <th>575</th>\n",
              "      <th>576</th>\n",
              "      <th>577</th>\n",
              "      <th>578</th>\n",
              "      <th>579</th>\n",
              "      <th>580</th>\n",
              "      <th>581</th>\n",
              "      <th>582</th>\n",
              "      <th>583</th>\n",
              "      <th>584</th>\n",
              "      <th>585</th>\n",
              "      <th>586</th>\n",
              "      <th>587</th>\n",
              "      <th>588</th>\n",
              "      <th>589</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1561.000000</td>\n",
              "      <td>1560.000000</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>1553.0</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>1558.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1564.0</td>\n",
              "      <td>1564.000000</td>\n",
              "      <td>1564.000000</td>\n",
              "      <td>1564.000000</td>\n",
              "      <td>1564.000000</td>\n",
              "      <td>1564.000000</td>\n",
              "      <td>1557.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1565.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1307.000000</td>\n",
              "      <td>1307.000000</td>\n",
              "      <td>1307.000000</td>\n",
              "      <td>1307.000000</td>\n",
              "      <td>1307.000000</td>\n",
              "      <td>1307.000000</td>\n",
              "      <td>1307.000000</td>\n",
              "      <td>1307.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1294.000000</td>\n",
              "      <td>1294.000000</td>\n",
              "      <td>1294.000000</td>\n",
              "      <td>1294.000000</td>\n",
              "      <td>1294.000000</td>\n",
              "      <td>1294.000000</td>\n",
              "      <td>1294.000000</td>\n",
              "      <td>1294.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>618.000000</td>\n",
              "      <td>618.000000</td>\n",
              "      <td>618.000000</td>\n",
              "      <td>618.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "      <td>1566.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.066369</td>\n",
              "      <td>3014.452896</td>\n",
              "      <td>2495.850231</td>\n",
              "      <td>2200.547318</td>\n",
              "      <td>1396.376627</td>\n",
              "      <td>4.197013</td>\n",
              "      <td>100.0</td>\n",
              "      <td>101.112908</td>\n",
              "      <td>0.121822</td>\n",
              "      <td>1.462862</td>\n",
              "      <td>-0.000841</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.964353</td>\n",
              "      <td>199.956809</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.005371</td>\n",
              "      <td>413.086035</td>\n",
              "      <td>9.907603</td>\n",
              "      <td>0.971444</td>\n",
              "      <td>190.047354</td>\n",
              "      <td>12.481034</td>\n",
              "      <td>1.405054</td>\n",
              "      <td>-5618.393610</td>\n",
              "      <td>2699.378435</td>\n",
              "      <td>-3806.299734</td>\n",
              "      <td>-298.598136</td>\n",
              "      <td>1.203845</td>\n",
              "      <td>1.938477</td>\n",
              "      <td>6.638628</td>\n",
              "      <td>69.499532</td>\n",
              "      <td>2.366197</td>\n",
              "      <td>0.184159</td>\n",
              "      <td>3.673189</td>\n",
              "      <td>85.337469</td>\n",
              "      <td>8.960279</td>\n",
              "      <td>50.582639</td>\n",
              "      <td>64.555787</td>\n",
              "      <td>49.417370</td>\n",
              "      <td>66.221274</td>\n",
              "      <td>86.836577</td>\n",
              "      <td>...</td>\n",
              "      <td>17.013313</td>\n",
              "      <td>1.230712</td>\n",
              "      <td>0.276688</td>\n",
              "      <td>7.703874</td>\n",
              "      <td>0.503657</td>\n",
              "      <td>57.746537</td>\n",
              "      <td>4.216905</td>\n",
              "      <td>1.623070</td>\n",
              "      <td>0.995009</td>\n",
              "      <td>0.325708</td>\n",
              "      <td>0.072443</td>\n",
              "      <td>32.284956</td>\n",
              "      <td>262.729683</td>\n",
              "      <td>0.679641</td>\n",
              "      <td>6.444985</td>\n",
              "      <td>0.145610</td>\n",
              "      <td>2.610870</td>\n",
              "      <td>0.060086</td>\n",
              "      <td>2.452417</td>\n",
              "      <td>21.117674</td>\n",
              "      <td>530.523623</td>\n",
              "      <td>2.101836</td>\n",
              "      <td>28.450165</td>\n",
              "      <td>0.345636</td>\n",
              "      <td>9.162315</td>\n",
              "      <td>0.104729</td>\n",
              "      <td>5.563747</td>\n",
              "      <td>16.642363</td>\n",
              "      <td>0.021615</td>\n",
              "      <td>0.016829</td>\n",
              "      <td>0.005396</td>\n",
              "      <td>97.934373</td>\n",
              "      <td>0.500096</td>\n",
              "      <td>0.015318</td>\n",
              "      <td>0.003847</td>\n",
              "      <td>3.067826</td>\n",
              "      <td>0.021458</td>\n",
              "      <td>0.016475</td>\n",
              "      <td>0.005283</td>\n",
              "      <td>99.670066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.249005</td>\n",
              "      <td>73.621787</td>\n",
              "      <td>80.407705</td>\n",
              "      <td>29.513152</td>\n",
              "      <td>441.691640</td>\n",
              "      <td>56.355540</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.237214</td>\n",
              "      <td>0.008961</td>\n",
              "      <td>0.073897</td>\n",
              "      <td>0.015116</td>\n",
              "      <td>0.009302</td>\n",
              "      <td>0.012452</td>\n",
              "      <td>3.257276</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.796596</td>\n",
              "      <td>17.221095</td>\n",
              "      <td>2.403867</td>\n",
              "      <td>0.012062</td>\n",
              "      <td>2.781041</td>\n",
              "      <td>0.217965</td>\n",
              "      <td>0.016737</td>\n",
              "      <td>626.822178</td>\n",
              "      <td>295.498535</td>\n",
              "      <td>1380.162148</td>\n",
              "      <td>2902.690117</td>\n",
              "      <td>0.177600</td>\n",
              "      <td>0.189495</td>\n",
              "      <td>1.244249</td>\n",
              "      <td>3.461181</td>\n",
              "      <td>0.408694</td>\n",
              "      <td>0.032944</td>\n",
              "      <td>0.535322</td>\n",
              "      <td>2.026549</td>\n",
              "      <td>1.344456</td>\n",
              "      <td>1.182618</td>\n",
              "      <td>2.574749</td>\n",
              "      <td>1.182619</td>\n",
              "      <td>0.304141</td>\n",
              "      <td>0.446756</td>\n",
              "      <td>...</td>\n",
              "      <td>4.966954</td>\n",
              "      <td>1.361117</td>\n",
              "      <td>0.276231</td>\n",
              "      <td>2.192647</td>\n",
              "      <td>0.598852</td>\n",
              "      <td>35.207552</td>\n",
              "      <td>1.280008</td>\n",
              "      <td>1.870433</td>\n",
              "      <td>0.083860</td>\n",
              "      <td>0.201392</td>\n",
              "      <td>0.051578</td>\n",
              "      <td>19.026081</td>\n",
              "      <td>7.630585</td>\n",
              "      <td>0.121758</td>\n",
              "      <td>2.633583</td>\n",
              "      <td>0.081122</td>\n",
              "      <td>1.032761</td>\n",
              "      <td>0.032761</td>\n",
              "      <td>0.996644</td>\n",
              "      <td>10.213294</td>\n",
              "      <td>17.499736</td>\n",
              "      <td>0.275112</td>\n",
              "      <td>86.304681</td>\n",
              "      <td>0.248478</td>\n",
              "      <td>26.920150</td>\n",
              "      <td>0.067791</td>\n",
              "      <td>16.921369</td>\n",
              "      <td>12.485267</td>\n",
              "      <td>0.011730</td>\n",
              "      <td>0.009640</td>\n",
              "      <td>0.003116</td>\n",
              "      <td>87.520966</td>\n",
              "      <td>0.003404</td>\n",
              "      <td>0.017180</td>\n",
              "      <td>0.003720</td>\n",
              "      <td>3.578033</td>\n",
              "      <td>0.012358</td>\n",
              "      <td>0.008808</td>\n",
              "      <td>0.002867</td>\n",
              "      <td>93.891919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2743.240000</td>\n",
              "      <td>2158.750000</td>\n",
              "      <td>2060.660000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.681500</td>\n",
              "      <td>100.0</td>\n",
              "      <td>82.131100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.191000</td>\n",
              "      <td>-0.053400</td>\n",
              "      <td>-0.034900</td>\n",
              "      <td>0.655400</td>\n",
              "      <td>182.094000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.249300</td>\n",
              "      <td>333.448600</td>\n",
              "      <td>4.469600</td>\n",
              "      <td>0.579400</td>\n",
              "      <td>169.177400</td>\n",
              "      <td>9.877300</td>\n",
              "      <td>1.179700</td>\n",
              "      <td>-7150.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9986.750000</td>\n",
              "      <td>-14804.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.400000</td>\n",
              "      <td>0.666700</td>\n",
              "      <td>0.034100</td>\n",
              "      <td>2.069800</td>\n",
              "      <td>83.182900</td>\n",
              "      <td>7.603200</td>\n",
              "      <td>49.834800</td>\n",
              "      <td>63.677400</td>\n",
              "      <td>40.228900</td>\n",
              "      <td>64.919300</td>\n",
              "      <td>84.732700</td>\n",
              "      <td>...</td>\n",
              "      <td>6.110000</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>0.018700</td>\n",
              "      <td>2.786000</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>4.826900</td>\n",
              "      <td>1.496700</td>\n",
              "      <td>0.164600</td>\n",
              "      <td>0.891900</td>\n",
              "      <td>0.069900</td>\n",
              "      <td>0.017700</td>\n",
              "      <td>7.236900</td>\n",
              "      <td>242.286000</td>\n",
              "      <td>0.304900</td>\n",
              "      <td>0.970000</td>\n",
              "      <td>0.022400</td>\n",
              "      <td>0.412200</td>\n",
              "      <td>0.009100</td>\n",
              "      <td>0.370600</td>\n",
              "      <td>3.250400</td>\n",
              "      <td>317.196400</td>\n",
              "      <td>0.980200</td>\n",
              "      <td>3.540000</td>\n",
              "      <td>0.066700</td>\n",
              "      <td>1.039500</td>\n",
              "      <td>0.023000</td>\n",
              "      <td>0.663600</td>\n",
              "      <td>4.582000</td>\n",
              "      <td>-0.016900</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.477800</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>1.197500</td>\n",
              "      <td>-0.016900</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2966.260000</td>\n",
              "      <td>2452.247500</td>\n",
              "      <td>2181.044400</td>\n",
              "      <td>1081.875800</td>\n",
              "      <td>1.017700</td>\n",
              "      <td>100.0</td>\n",
              "      <td>97.920000</td>\n",
              "      <td>0.121100</td>\n",
              "      <td>1.411200</td>\n",
              "      <td>-0.010800</td>\n",
              "      <td>-0.005600</td>\n",
              "      <td>0.958100</td>\n",
              "      <td>198.130700</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.094875</td>\n",
              "      <td>406.127400</td>\n",
              "      <td>9.567625</td>\n",
              "      <td>0.968200</td>\n",
              "      <td>188.299825</td>\n",
              "      <td>12.460000</td>\n",
              "      <td>1.396500</td>\n",
              "      <td>-5933.250000</td>\n",
              "      <td>2578.000000</td>\n",
              "      <td>-4371.750000</td>\n",
              "      <td>-1476.000000</td>\n",
              "      <td>1.094800</td>\n",
              "      <td>1.906500</td>\n",
              "      <td>5.263700</td>\n",
              "      <td>67.377800</td>\n",
              "      <td>2.088900</td>\n",
              "      <td>0.161700</td>\n",
              "      <td>3.362700</td>\n",
              "      <td>84.490500</td>\n",
              "      <td>8.580000</td>\n",
              "      <td>50.252350</td>\n",
              "      <td>64.024800</td>\n",
              "      <td>49.421200</td>\n",
              "      <td>66.040650</td>\n",
              "      <td>86.578300</td>\n",
              "      <td>...</td>\n",
              "      <td>14.530000</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.094900</td>\n",
              "      <td>6.738100</td>\n",
              "      <td>0.343800</td>\n",
              "      <td>27.017600</td>\n",
              "      <td>3.625100</td>\n",
              "      <td>1.182900</td>\n",
              "      <td>0.955200</td>\n",
              "      <td>0.149825</td>\n",
              "      <td>0.036200</td>\n",
              "      <td>15.762450</td>\n",
              "      <td>259.972500</td>\n",
              "      <td>0.567100</td>\n",
              "      <td>4.980000</td>\n",
              "      <td>0.087700</td>\n",
              "      <td>2.090200</td>\n",
              "      <td>0.038200</td>\n",
              "      <td>1.884400</td>\n",
              "      <td>15.466200</td>\n",
              "      <td>530.702700</td>\n",
              "      <td>1.982900</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>0.242250</td>\n",
              "      <td>2.567850</td>\n",
              "      <td>0.075100</td>\n",
              "      <td>1.408450</td>\n",
              "      <td>11.501550</td>\n",
              "      <td>0.013800</td>\n",
              "      <td>0.010600</td>\n",
              "      <td>0.003400</td>\n",
              "      <td>46.184900</td>\n",
              "      <td>0.497900</td>\n",
              "      <td>0.011600</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>2.306500</td>\n",
              "      <td>0.013425</td>\n",
              "      <td>0.010600</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>44.368600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>3011.490000</td>\n",
              "      <td>2499.405000</td>\n",
              "      <td>2201.066700</td>\n",
              "      <td>1285.214400</td>\n",
              "      <td>1.316800</td>\n",
              "      <td>100.0</td>\n",
              "      <td>101.512200</td>\n",
              "      <td>0.122400</td>\n",
              "      <td>1.461600</td>\n",
              "      <td>-0.001300</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.965800</td>\n",
              "      <td>199.535600</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.967000</td>\n",
              "      <td>412.219100</td>\n",
              "      <td>9.851750</td>\n",
              "      <td>0.972600</td>\n",
              "      <td>189.664200</td>\n",
              "      <td>12.499600</td>\n",
              "      <td>1.406000</td>\n",
              "      <td>-5523.250000</td>\n",
              "      <td>2664.000000</td>\n",
              "      <td>-3820.750000</td>\n",
              "      <td>-78.750000</td>\n",
              "      <td>1.283000</td>\n",
              "      <td>1.986500</td>\n",
              "      <td>7.264700</td>\n",
              "      <td>69.155600</td>\n",
              "      <td>2.377800</td>\n",
              "      <td>0.186700</td>\n",
              "      <td>3.431000</td>\n",
              "      <td>85.135450</td>\n",
              "      <td>8.769800</td>\n",
              "      <td>50.396400</td>\n",
              "      <td>64.165800</td>\n",
              "      <td>49.603600</td>\n",
              "      <td>66.231800</td>\n",
              "      <td>86.820700</td>\n",
              "      <td>...</td>\n",
              "      <td>16.340000</td>\n",
              "      <td>1.150000</td>\n",
              "      <td>0.197900</td>\n",
              "      <td>7.427900</td>\n",
              "      <td>0.478900</td>\n",
              "      <td>54.441700</td>\n",
              "      <td>4.067100</td>\n",
              "      <td>1.529800</td>\n",
              "      <td>0.972700</td>\n",
              "      <td>0.290900</td>\n",
              "      <td>0.059200</td>\n",
              "      <td>29.731150</td>\n",
              "      <td>264.272000</td>\n",
              "      <td>0.651000</td>\n",
              "      <td>5.160000</td>\n",
              "      <td>0.119550</td>\n",
              "      <td>2.150450</td>\n",
              "      <td>0.048650</td>\n",
              "      <td>1.999700</td>\n",
              "      <td>16.988350</td>\n",
              "      <td>532.398200</td>\n",
              "      <td>2.118600</td>\n",
              "      <td>8.650000</td>\n",
              "      <td>0.293400</td>\n",
              "      <td>2.975800</td>\n",
              "      <td>0.089500</td>\n",
              "      <td>1.624500</td>\n",
              "      <td>13.817900</td>\n",
              "      <td>0.020400</td>\n",
              "      <td>0.014800</td>\n",
              "      <td>0.004700</td>\n",
              "      <td>72.288900</td>\n",
              "      <td>0.500200</td>\n",
              "      <td>0.013800</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>2.757650</td>\n",
              "      <td>0.020500</td>\n",
              "      <td>0.014800</td>\n",
              "      <td>0.004600</td>\n",
              "      <td>71.900500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>3056.650000</td>\n",
              "      <td>2538.822500</td>\n",
              "      <td>2218.055500</td>\n",
              "      <td>1591.223500</td>\n",
              "      <td>1.525700</td>\n",
              "      <td>100.0</td>\n",
              "      <td>104.586700</td>\n",
              "      <td>0.123800</td>\n",
              "      <td>1.516900</td>\n",
              "      <td>0.008400</td>\n",
              "      <td>0.005900</td>\n",
              "      <td>0.971300</td>\n",
              "      <td>202.007100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.861875</td>\n",
              "      <td>419.089275</td>\n",
              "      <td>10.128175</td>\n",
              "      <td>0.976800</td>\n",
              "      <td>192.189375</td>\n",
              "      <td>12.547100</td>\n",
              "      <td>1.415000</td>\n",
              "      <td>-5356.250000</td>\n",
              "      <td>2841.750000</td>\n",
              "      <td>-3352.750000</td>\n",
              "      <td>1377.250000</td>\n",
              "      <td>1.304300</td>\n",
              "      <td>2.003200</td>\n",
              "      <td>7.329700</td>\n",
              "      <td>72.266700</td>\n",
              "      <td>2.655600</td>\n",
              "      <td>0.207100</td>\n",
              "      <td>3.531300</td>\n",
              "      <td>85.741900</td>\n",
              "      <td>9.060600</td>\n",
              "      <td>50.578800</td>\n",
              "      <td>64.344700</td>\n",
              "      <td>49.747650</td>\n",
              "      <td>66.343275</td>\n",
              "      <td>87.002400</td>\n",
              "      <td>...</td>\n",
              "      <td>19.035000</td>\n",
              "      <td>1.370000</td>\n",
              "      <td>0.358450</td>\n",
              "      <td>8.637150</td>\n",
              "      <td>0.562350</td>\n",
              "      <td>74.628700</td>\n",
              "      <td>4.702700</td>\n",
              "      <td>1.815600</td>\n",
              "      <td>1.000800</td>\n",
              "      <td>0.443600</td>\n",
              "      <td>0.089000</td>\n",
              "      <td>44.113400</td>\n",
              "      <td>265.707000</td>\n",
              "      <td>0.768875</td>\n",
              "      <td>7.800000</td>\n",
              "      <td>0.186150</td>\n",
              "      <td>3.098725</td>\n",
              "      <td>0.075275</td>\n",
              "      <td>2.970850</td>\n",
              "      <td>24.772175</td>\n",
              "      <td>534.356400</td>\n",
              "      <td>2.290650</td>\n",
              "      <td>10.130000</td>\n",
              "      <td>0.366900</td>\n",
              "      <td>3.492500</td>\n",
              "      <td>0.112150</td>\n",
              "      <td>1.902000</td>\n",
              "      <td>17.080900</td>\n",
              "      <td>0.027700</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.006475</td>\n",
              "      <td>116.539150</td>\n",
              "      <td>0.502375</td>\n",
              "      <td>0.016500</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>3.295175</td>\n",
              "      <td>0.027600</td>\n",
              "      <td>0.020300</td>\n",
              "      <td>0.006400</td>\n",
              "      <td>114.749700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>3356.350000</td>\n",
              "      <td>2846.440000</td>\n",
              "      <td>2315.266700</td>\n",
              "      <td>3715.041700</td>\n",
              "      <td>1114.536600</td>\n",
              "      <td>100.0</td>\n",
              "      <td>129.252200</td>\n",
              "      <td>0.128600</td>\n",
              "      <td>1.656400</td>\n",
              "      <td>0.074900</td>\n",
              "      <td>0.053000</td>\n",
              "      <td>0.984800</td>\n",
              "      <td>272.045100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.546500</td>\n",
              "      <td>824.927100</td>\n",
              "      <td>102.867700</td>\n",
              "      <td>0.984800</td>\n",
              "      <td>215.597700</td>\n",
              "      <td>12.989800</td>\n",
              "      <td>1.453400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3656.250000</td>\n",
              "      <td>2363.000000</td>\n",
              "      <td>14106.000000</td>\n",
              "      <td>1.382800</td>\n",
              "      <td>2.052800</td>\n",
              "      <td>7.658800</td>\n",
              "      <td>77.900000</td>\n",
              "      <td>3.511100</td>\n",
              "      <td>0.285100</td>\n",
              "      <td>4.804400</td>\n",
              "      <td>105.603800</td>\n",
              "      <td>23.345300</td>\n",
              "      <td>59.771100</td>\n",
              "      <td>94.264100</td>\n",
              "      <td>50.165200</td>\n",
              "      <td>67.958600</td>\n",
              "      <td>88.418800</td>\n",
              "      <td>...</td>\n",
              "      <td>131.680000</td>\n",
              "      <td>39.330000</td>\n",
              "      <td>2.718200</td>\n",
              "      <td>56.930300</td>\n",
              "      <td>17.478100</td>\n",
              "      <td>303.550000</td>\n",
              "      <td>35.319800</td>\n",
              "      <td>54.291700</td>\n",
              "      <td>1.512100</td>\n",
              "      <td>1.073700</td>\n",
              "      <td>0.445700</td>\n",
              "      <td>101.114600</td>\n",
              "      <td>311.404000</td>\n",
              "      <td>1.298800</td>\n",
              "      <td>32.580000</td>\n",
              "      <td>0.689200</td>\n",
              "      <td>14.014100</td>\n",
              "      <td>0.293200</td>\n",
              "      <td>12.746200</td>\n",
              "      <td>84.802400</td>\n",
              "      <td>589.508200</td>\n",
              "      <td>2.739500</td>\n",
              "      <td>454.560000</td>\n",
              "      <td>2.196700</td>\n",
              "      <td>170.020400</td>\n",
              "      <td>0.550200</td>\n",
              "      <td>90.423500</td>\n",
              "      <td>96.960100</td>\n",
              "      <td>0.102800</td>\n",
              "      <td>0.079900</td>\n",
              "      <td>0.028600</td>\n",
              "      <td>737.304800</td>\n",
              "      <td>0.509800</td>\n",
              "      <td>0.476600</td>\n",
              "      <td>0.104500</td>\n",
              "      <td>99.303200</td>\n",
              "      <td>0.102800</td>\n",
              "      <td>0.079900</td>\n",
              "      <td>0.028600</td>\n",
              "      <td>737.304800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 591 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          passFail            0  ...          588          589\n",
              "count  1567.000000  1561.000000  ...  1566.000000  1566.000000\n",
              "mean      0.066369  3014.452896  ...     0.005283    99.670066\n",
              "std       0.249005    73.621787  ...     0.002867    93.891919\n",
              "min       0.000000  2743.240000  ...     0.001000     0.000000\n",
              "25%       0.000000  2966.260000  ...     0.003300    44.368600\n",
              "50%       0.000000  3011.490000  ...     0.004600    71.900500\n",
              "75%       0.000000  3056.650000  ...     0.006400   114.749700\n",
              "max       1.000000  3356.350000  ...     0.028600   737.304800\n",
              "\n",
              "[8 rows x 591 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt3-GoIeymsw",
        "outputId": "132e9480-a753-492d-b291-c70c0d84b23e"
      },
      "source": [
        "secomMerged.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1567 entries, 0 to 1566\n",
            "Columns: 592 entries, passFail to 589\n",
            "dtypes: datetime64[ns](1), float64(590), int64(1)\n",
            "memory usage: 7.1 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H4Czz_jOv5-"
      },
      "source": [
        "#looking to take care of NaN/ missing values.\n",
        "#secomMerged.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "AaX5B8lmOzwu",
        "outputId": "a546cf4c-2935-47e5-b76a-ab9006398481"
      },
      "source": [
        "# histogram of counts of number of NaN values in each column\n",
        "import matplotlib.pyplot as plt\n",
        "secomMerged.isna().sum().hist()\n",
        "plt.title(\"Count of NaN in each Column\")\n",
        "plt.xlabel(\"Number of NaN in Column\")\n",
        "plt.ylabel(\"Number of columns\")\n",
        "print(f'Number of columns with more than 250 NaN: {sum(secomMerged.isna().sum() > 250)}')\n",
        "totNumNaN = secomMerged.isna().sum().sum()\n",
        "totNumValues = len(secomMerged) * (len(secomMerged.columns) - 2)\n",
        "print(f'Total number of NaN in all columns: {totNumNaN}')\n",
        "print(f'Percent of NaN out of all data:     {100*totNumNaN/totNumValues:0.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of columns with more than 250 NaN: 52\n",
            "Total number of NaN in all columns: 41951\n",
            "Percent of NaN out of all data:     4.54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH-rSgOKO5BN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c5d846-4d18-4857-8f8c-d81dd5706830"
      },
      "source": [
        "# Do not fill the NaN with mean!  Missing NaN should be zero, because\n",
        "# there is no data there; i.e., no sensor data\n",
        "#secomMerged.fillna(secomMerged.mean(), inplace=True)\n",
        "secomMerged  = secomMerged.replace(np.NaN, 0)  # replace NaN with zero\n",
        "totNumNaN = secomMerged.isna().sum().sum()\n",
        "print(f'Total number of NaN in all columns: {totNumNaN}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of NaN in all columns: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "8ybiELHwO5fs",
        "outputId": "e827ac3a-ce8b-429f-f2c0-c8d595a2ddc8"
      },
      "source": [
        "secomMerged.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passFail</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>550</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "      <th>561</th>\n",
              "      <th>562</th>\n",
              "      <th>563</th>\n",
              "      <th>564</th>\n",
              "      <th>565</th>\n",
              "      <th>566</th>\n",
              "      <th>567</th>\n",
              "      <th>568</th>\n",
              "      <th>569</th>\n",
              "      <th>570</th>\n",
              "      <th>571</th>\n",
              "      <th>572</th>\n",
              "      <th>573</th>\n",
              "      <th>574</th>\n",
              "      <th>575</th>\n",
              "      <th>576</th>\n",
              "      <th>577</th>\n",
              "      <th>578</th>\n",
              "      <th>579</th>\n",
              "      <th>580</th>\n",
              "      <th>581</th>\n",
              "      <th>582</th>\n",
              "      <th>583</th>\n",
              "      <th>584</th>\n",
              "      <th>585</th>\n",
              "      <th>586</th>\n",
              "      <th>587</th>\n",
              "      <th>588</th>\n",
              "      <th>589</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.0</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "      <td>1567.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.066369</td>\n",
              "      <td>3002.910638</td>\n",
              "      <td>2484.700932</td>\n",
              "      <td>2180.887035</td>\n",
              "      <td>1383.901023</td>\n",
              "      <td>4.159516</td>\n",
              "      <td>99.106573</td>\n",
              "      <td>100.209538</td>\n",
              "      <td>0.121122</td>\n",
              "      <td>1.460995</td>\n",
              "      <td>-0.000840</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.963122</td>\n",
              "      <td>199.701600</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.988130</td>\n",
              "      <td>412.295188</td>\n",
              "      <td>9.888635</td>\n",
              "      <td>0.969584</td>\n",
              "      <td>189.683511</td>\n",
              "      <td>12.401385</td>\n",
              "      <td>1.405054</td>\n",
              "      <td>-5611.222719</td>\n",
              "      <td>2695.933153</td>\n",
              "      <td>-3801.441661</td>\n",
              "      <td>-298.217028</td>\n",
              "      <td>1.202308</td>\n",
              "      <td>1.936003</td>\n",
              "      <td>6.630155</td>\n",
              "      <td>69.410828</td>\n",
              "      <td>2.363177</td>\n",
              "      <td>0.183924</td>\n",
              "      <td>3.668501</td>\n",
              "      <td>85.283010</td>\n",
              "      <td>8.954560</td>\n",
              "      <td>50.550359</td>\n",
              "      <td>64.514590</td>\n",
              "      <td>49.385834</td>\n",
              "      <td>66.179014</td>\n",
              "      <td>86.781161</td>\n",
              "      <td>...</td>\n",
              "      <td>14.190428</td>\n",
              "      <td>1.026509</td>\n",
              "      <td>0.230779</td>\n",
              "      <td>6.425630</td>\n",
              "      <td>0.420090</td>\n",
              "      <td>48.165108</td>\n",
              "      <td>3.517227</td>\n",
              "      <td>1.353767</td>\n",
              "      <td>0.994374</td>\n",
              "      <td>0.325500</td>\n",
              "      <td>0.072397</td>\n",
              "      <td>32.264353</td>\n",
              "      <td>216.957377</td>\n",
              "      <td>0.561235</td>\n",
              "      <td>5.322151</td>\n",
              "      <td>0.120242</td>\n",
              "      <td>2.156008</td>\n",
              "      <td>0.049618</td>\n",
              "      <td>2.025161</td>\n",
              "      <td>17.438590</td>\n",
              "      <td>530.523623</td>\n",
              "      <td>2.101836</td>\n",
              "      <td>28.450165</td>\n",
              "      <td>0.345636</td>\n",
              "      <td>9.162315</td>\n",
              "      <td>0.104729</td>\n",
              "      <td>5.563747</td>\n",
              "      <td>16.642363</td>\n",
              "      <td>0.008524</td>\n",
              "      <td>0.006637</td>\n",
              "      <td>0.002128</td>\n",
              "      <td>38.623767</td>\n",
              "      <td>0.499777</td>\n",
              "      <td>0.015308</td>\n",
              "      <td>0.003844</td>\n",
              "      <td>3.065869</td>\n",
              "      <td>0.021445</td>\n",
              "      <td>0.016464</td>\n",
              "      <td>0.005280</td>\n",
              "      <td>99.606461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.249005</td>\n",
              "      <td>200.204648</td>\n",
              "      <td>184.815753</td>\n",
              "      <td>209.206773</td>\n",
              "      <td>458.937272</td>\n",
              "      <td>56.104457</td>\n",
              "      <td>9.412812</td>\n",
              "      <td>11.363940</td>\n",
              "      <td>0.012831</td>\n",
              "      <td>0.090461</td>\n",
              "      <td>0.015107</td>\n",
              "      <td>0.009296</td>\n",
              "      <td>0.036620</td>\n",
              "      <td>7.848224</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.821529</td>\n",
              "      <td>24.945317</td>\n",
              "      <td>2.440326</td>\n",
              "      <td>0.044155</td>\n",
              "      <td>8.762332</td>\n",
              "      <td>1.017643</td>\n",
              "      <td>0.016737</td>\n",
              "      <td>657.774591</td>\n",
              "      <td>310.647785</td>\n",
              "      <td>1385.963301</td>\n",
              "      <td>2900.855558</td>\n",
              "      <td>0.182620</td>\n",
              "      <td>0.201632</td>\n",
              "      <td>1.265856</td>\n",
              "      <td>4.257396</td>\n",
              "      <td>0.417084</td>\n",
              "      <td>0.033573</td>\n",
              "      <td>0.550829</td>\n",
              "      <td>2.958325</td>\n",
              "      <td>1.362954</td>\n",
              "      <td>1.740832</td>\n",
              "      <td>3.047065</td>\n",
              "      <td>1.719341</td>\n",
              "      <td>1.700279</td>\n",
              "      <td>2.238656</td>\n",
              "      <td>...</td>\n",
              "      <td>7.788334</td>\n",
              "      <td>1.324688</td>\n",
              "      <td>0.272464</td>\n",
              "      <td>3.496892</td>\n",
              "      <td>0.578109</td>\n",
              "      <td>38.672384</td>\n",
              "      <td>1.956756</td>\n",
              "      <td>1.811760</td>\n",
              "      <td>0.087520</td>\n",
              "      <td>0.201496</td>\n",
              "      <td>0.051594</td>\n",
              "      <td>19.037484</td>\n",
              "      <td>99.925253</td>\n",
              "      <td>0.280600</td>\n",
              "      <td>3.421456</td>\n",
              "      <td>0.092119</td>\n",
              "      <td>1.364539</td>\n",
              "      <td>0.037496</td>\n",
              "      <td>1.298442</td>\n",
              "      <td>12.260744</td>\n",
              "      <td>17.499736</td>\n",
              "      <td>0.275112</td>\n",
              "      <td>86.304681</td>\n",
              "      <td>0.248478</td>\n",
              "      <td>26.920150</td>\n",
              "      <td>0.067791</td>\n",
              "      <td>16.921369</td>\n",
              "      <td>12.485267</td>\n",
              "      <td>0.012879</td>\n",
              "      <td>0.010213</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>72.871466</td>\n",
              "      <td>0.013084</td>\n",
              "      <td>0.017179</td>\n",
              "      <td>0.003721</td>\n",
              "      <td>3.577730</td>\n",
              "      <td>0.012366</td>\n",
              "      <td>0.008815</td>\n",
              "      <td>0.002869</td>\n",
              "      <td>93.895701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.053400</td>\n",
              "      <td>-0.034900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.179700</td>\n",
              "      <td>-7150.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9986.750000</td>\n",
              "      <td>-14804.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>317.196400</td>\n",
              "      <td>0.980200</td>\n",
              "      <td>3.540000</td>\n",
              "      <td>0.066700</td>\n",
              "      <td>1.039500</td>\n",
              "      <td>0.023000</td>\n",
              "      <td>0.663600</td>\n",
              "      <td>4.582000</td>\n",
              "      <td>-0.016900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.016900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2965.670000</td>\n",
              "      <td>2451.515000</td>\n",
              "      <td>2180.700000</td>\n",
              "      <td>1080.116050</td>\n",
              "      <td>1.011000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>97.762200</td>\n",
              "      <td>0.121100</td>\n",
              "      <td>1.410950</td>\n",
              "      <td>-0.010800</td>\n",
              "      <td>-0.005600</td>\n",
              "      <td>0.958000</td>\n",
              "      <td>198.128450</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.080300</td>\n",
              "      <td>406.092900</td>\n",
              "      <td>9.565100</td>\n",
              "      <td>0.968100</td>\n",
              "      <td>188.291900</td>\n",
              "      <td>12.460000</td>\n",
              "      <td>1.396500</td>\n",
              "      <td>-5932.625000</td>\n",
              "      <td>2577.875000</td>\n",
              "      <td>-4370.625000</td>\n",
              "      <td>-1474.375000</td>\n",
              "      <td>1.093900</td>\n",
              "      <td>1.906150</td>\n",
              "      <td>5.262400</td>\n",
              "      <td>67.377800</td>\n",
              "      <td>2.088900</td>\n",
              "      <td>0.161700</td>\n",
              "      <td>3.362300</td>\n",
              "      <td>84.484350</td>\n",
              "      <td>8.580000</td>\n",
              "      <td>50.251500</td>\n",
              "      <td>64.024800</td>\n",
              "      <td>49.420500</td>\n",
              "      <td>66.040500</td>\n",
              "      <td>86.578300</td>\n",
              "      <td>...</td>\n",
              "      <td>13.415000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.065800</td>\n",
              "      <td>6.070750</td>\n",
              "      <td>0.273350</td>\n",
              "      <td>21.344000</td>\n",
              "      <td>3.318350</td>\n",
              "      <td>0.896550</td>\n",
              "      <td>0.955200</td>\n",
              "      <td>0.149700</td>\n",
              "      <td>0.036200</td>\n",
              "      <td>15.740700</td>\n",
              "      <td>250.420000</td>\n",
              "      <td>0.567100</td>\n",
              "      <td>4.045000</td>\n",
              "      <td>0.087700</td>\n",
              "      <td>1.658750</td>\n",
              "      <td>0.036600</td>\n",
              "      <td>1.549000</td>\n",
              "      <td>12.593550</td>\n",
              "      <td>530.702700</td>\n",
              "      <td>1.982900</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>0.242250</td>\n",
              "      <td>2.567850</td>\n",
              "      <td>0.075100</td>\n",
              "      <td>1.408450</td>\n",
              "      <td>11.501550</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.497900</td>\n",
              "      <td>0.011600</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>2.306200</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.010600</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>44.368600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>3010.920000</td>\n",
              "      <td>2498.910000</td>\n",
              "      <td>2200.955600</td>\n",
              "      <td>1283.436800</td>\n",
              "      <td>1.310100</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>101.492200</td>\n",
              "      <td>0.122400</td>\n",
              "      <td>1.461500</td>\n",
              "      <td>-0.001300</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.965800</td>\n",
              "      <td>199.525100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.956500</td>\n",
              "      <td>412.198800</td>\n",
              "      <td>9.850800</td>\n",
              "      <td>0.972500</td>\n",
              "      <td>189.661500</td>\n",
              "      <td>12.499600</td>\n",
              "      <td>1.406000</td>\n",
              "      <td>-5523.000000</td>\n",
              "      <td>2663.750000</td>\n",
              "      <td>-3819.750000</td>\n",
              "      <td>-74.000000</td>\n",
              "      <td>1.283000</td>\n",
              "      <td>1.986300</td>\n",
              "      <td>7.264500</td>\n",
              "      <td>69.144400</td>\n",
              "      <td>2.377800</td>\n",
              "      <td>0.186700</td>\n",
              "      <td>3.430700</td>\n",
              "      <td>85.130500</td>\n",
              "      <td>8.769600</td>\n",
              "      <td>50.396400</td>\n",
              "      <td>64.165800</td>\n",
              "      <td>49.603600</td>\n",
              "      <td>66.231400</td>\n",
              "      <td>86.820700</td>\n",
              "      <td>...</td>\n",
              "      <td>15.910000</td>\n",
              "      <td>1.020000</td>\n",
              "      <td>0.160600</td>\n",
              "      <td>7.094100</td>\n",
              "      <td>0.417200</td>\n",
              "      <td>45.838000</td>\n",
              "      <td>3.926300</td>\n",
              "      <td>1.360500</td>\n",
              "      <td>0.972700</td>\n",
              "      <td>0.290900</td>\n",
              "      <td>0.059200</td>\n",
              "      <td>29.682200</td>\n",
              "      <td>264.272000</td>\n",
              "      <td>0.602300</td>\n",
              "      <td>4.980000</td>\n",
              "      <td>0.090300</td>\n",
              "      <td>2.090200</td>\n",
              "      <td>0.038200</td>\n",
              "      <td>1.884400</td>\n",
              "      <td>15.466200</td>\n",
              "      <td>532.398200</td>\n",
              "      <td>2.118600</td>\n",
              "      <td>8.650000</td>\n",
              "      <td>0.293400</td>\n",
              "      <td>2.975800</td>\n",
              "      <td>0.089500</td>\n",
              "      <td>1.624500</td>\n",
              "      <td>13.817900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500200</td>\n",
              "      <td>0.013800</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>2.757600</td>\n",
              "      <td>0.020500</td>\n",
              "      <td>0.014800</td>\n",
              "      <td>0.004600</td>\n",
              "      <td>71.778000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>3056.540000</td>\n",
              "      <td>2538.745000</td>\n",
              "      <td>2218.055500</td>\n",
              "      <td>1590.169900</td>\n",
              "      <td>1.518800</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>104.530000</td>\n",
              "      <td>0.123800</td>\n",
              "      <td>1.516850</td>\n",
              "      <td>0.008400</td>\n",
              "      <td>0.005900</td>\n",
              "      <td>0.971300</td>\n",
              "      <td>202.006750</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.858700</td>\n",
              "      <td>419.082800</td>\n",
              "      <td>10.127750</td>\n",
              "      <td>0.976800</td>\n",
              "      <td>192.178900</td>\n",
              "      <td>12.547100</td>\n",
              "      <td>1.415000</td>\n",
              "      <td>-5353.500000</td>\n",
              "      <td>2840.625000</td>\n",
              "      <td>-3344.750000</td>\n",
              "      <td>1376.250000</td>\n",
              "      <td>1.304300</td>\n",
              "      <td>2.003200</td>\n",
              "      <td>7.329600</td>\n",
              "      <td>72.255550</td>\n",
              "      <td>2.655600</td>\n",
              "      <td>0.207000</td>\n",
              "      <td>3.531250</td>\n",
              "      <td>85.741900</td>\n",
              "      <td>9.060600</td>\n",
              "      <td>50.578100</td>\n",
              "      <td>64.344700</td>\n",
              "      <td>49.747100</td>\n",
              "      <td>66.343050</td>\n",
              "      <td>87.002400</td>\n",
              "      <td>...</td>\n",
              "      <td>18.405000</td>\n",
              "      <td>1.330000</td>\n",
              "      <td>0.321750</td>\n",
              "      <td>8.316650</td>\n",
              "      <td>0.533550</td>\n",
              "      <td>69.630650</td>\n",
              "      <td>4.564950</td>\n",
              "      <td>1.815600</td>\n",
              "      <td>1.000800</td>\n",
              "      <td>0.443600</td>\n",
              "      <td>0.089000</td>\n",
              "      <td>44.113400</td>\n",
              "      <td>264.733000</td>\n",
              "      <td>0.738250</td>\n",
              "      <td>7.310000</td>\n",
              "      <td>0.166850</td>\n",
              "      <td>2.909150</td>\n",
              "      <td>0.068100</td>\n",
              "      <td>2.807750</td>\n",
              "      <td>23.035200</td>\n",
              "      <td>534.356400</td>\n",
              "      <td>2.290650</td>\n",
              "      <td>10.130000</td>\n",
              "      <td>0.366900</td>\n",
              "      <td>3.492500</td>\n",
              "      <td>0.112150</td>\n",
              "      <td>1.902000</td>\n",
              "      <td>17.080900</td>\n",
              "      <td>0.017350</td>\n",
              "      <td>0.012300</td>\n",
              "      <td>0.003900</td>\n",
              "      <td>57.449750</td>\n",
              "      <td>0.502350</td>\n",
              "      <td>0.016500</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>3.294950</td>\n",
              "      <td>0.027600</td>\n",
              "      <td>0.020300</td>\n",
              "      <td>0.006400</td>\n",
              "      <td>114.749700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>3356.350000</td>\n",
              "      <td>2846.440000</td>\n",
              "      <td>2315.266700</td>\n",
              "      <td>3715.041700</td>\n",
              "      <td>1114.536600</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>129.252200</td>\n",
              "      <td>0.128600</td>\n",
              "      <td>1.656400</td>\n",
              "      <td>0.074900</td>\n",
              "      <td>0.053000</td>\n",
              "      <td>0.984800</td>\n",
              "      <td>272.045100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.546500</td>\n",
              "      <td>824.927100</td>\n",
              "      <td>102.867700</td>\n",
              "      <td>0.984800</td>\n",
              "      <td>215.597700</td>\n",
              "      <td>12.989800</td>\n",
              "      <td>1.453400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3656.250000</td>\n",
              "      <td>2363.000000</td>\n",
              "      <td>14106.000000</td>\n",
              "      <td>1.382800</td>\n",
              "      <td>2.052800</td>\n",
              "      <td>7.658800</td>\n",
              "      <td>77.900000</td>\n",
              "      <td>3.511100</td>\n",
              "      <td>0.285100</td>\n",
              "      <td>4.804400</td>\n",
              "      <td>105.603800</td>\n",
              "      <td>23.345300</td>\n",
              "      <td>59.771100</td>\n",
              "      <td>94.264100</td>\n",
              "      <td>50.165200</td>\n",
              "      <td>67.958600</td>\n",
              "      <td>88.418800</td>\n",
              "      <td>...</td>\n",
              "      <td>131.680000</td>\n",
              "      <td>39.330000</td>\n",
              "      <td>2.718200</td>\n",
              "      <td>56.930300</td>\n",
              "      <td>17.478100</td>\n",
              "      <td>303.550000</td>\n",
              "      <td>35.319800</td>\n",
              "      <td>54.291700</td>\n",
              "      <td>1.512100</td>\n",
              "      <td>1.073700</td>\n",
              "      <td>0.445700</td>\n",
              "      <td>101.114600</td>\n",
              "      <td>311.404000</td>\n",
              "      <td>1.298800</td>\n",
              "      <td>32.580000</td>\n",
              "      <td>0.689200</td>\n",
              "      <td>14.014100</td>\n",
              "      <td>0.293200</td>\n",
              "      <td>12.746200</td>\n",
              "      <td>84.802400</td>\n",
              "      <td>589.508200</td>\n",
              "      <td>2.739500</td>\n",
              "      <td>454.560000</td>\n",
              "      <td>2.196700</td>\n",
              "      <td>170.020400</td>\n",
              "      <td>0.550200</td>\n",
              "      <td>90.423500</td>\n",
              "      <td>96.960100</td>\n",
              "      <td>0.102800</td>\n",
              "      <td>0.079900</td>\n",
              "      <td>0.028600</td>\n",
              "      <td>737.304800</td>\n",
              "      <td>0.509800</td>\n",
              "      <td>0.476600</td>\n",
              "      <td>0.104500</td>\n",
              "      <td>99.303200</td>\n",
              "      <td>0.102800</td>\n",
              "      <td>0.079900</td>\n",
              "      <td>0.028600</td>\n",
              "      <td>737.304800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 591 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          passFail            0  ...          588          589\n",
              "count  1567.000000  1567.000000  ...  1567.000000  1567.000000\n",
              "mean      0.066369  3002.910638  ...     0.005280    99.606461\n",
              "std       0.249005   200.204648  ...     0.002869    93.895701\n",
              "min       0.000000     0.000000  ...     0.000000     0.000000\n",
              "25%       0.000000  2965.670000  ...     0.003300    44.368600\n",
              "50%       0.000000  3010.920000  ...     0.004600    71.778000\n",
              "75%       0.000000  3056.540000  ...     0.006400   114.749700\n",
              "max       1.000000  3356.350000  ...     0.028600   737.304800\n",
              "\n",
              "[8 rows x 591 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22r8QfgVLEWG",
        "outputId": "2e007c59-967a-4d68-ebb7-efcafd3b174d"
      },
      "source": [
        "# how many of each datatype columns do we have?\n",
        "secomMerged.dtypes.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "float64           590\n",
              "int64               1\n",
              "datetime64[ns]      1\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRGFfxXTLvJH"
      },
      "source": [
        "#These steps are now unnecessary because we now correctly read in the secom.data csv file.   \n",
        "# allCols = set(secomMerged.columns)\n",
        "# print(f'Number of total columns: {len(allCols)}')\n",
        "# numericCols = sorted( allCols - set(['passFail', 'datetime']) )\n",
        "# print(f'Number of numeric columns: {len(numericCols)}')\n",
        "# # secomMerged2 = pd.to_numeric(secomMerged[ numericCols ], errors='coerce')\n",
        "# secomMerged2 = secomMerged[ numericCols ].apply(pd.to_numeric, errors='coerce')\n",
        "# secomMerged2['passFail'] = secomMerged['passFail'].astype('int64')\n",
        "# secomMerged2['dateStamp'] = secomMerged['dateStamp'].str.replace(r'\"', '').apply(parse)\n",
        "# secomMerged2['timeStamp'] = secomMerged['timeStamp'].str.replace(r'\"', '') #.apply(lambda x: parse(x).time)\n",
        "# secomMerged2 = secomMerged2.loc[:, ['passFail','dateStamp','timeStamp'] + numericCols ]\n",
        "# secomMerged2.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCC7gHltjJqr"
      },
      "source": [
        "The NaNs in this dataset were stuborn. Below we take care of them using code adapted from:\n",
        "\n",
        "https://www.kaggle.com/kuldeepsingharya/uci-secom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2m6RIq5LX-A"
      },
      "source": [
        "#These steps are now unnecessary because we now correctly read in the secom.data csv file.   \n",
        "# np.nan_to_num(secomMerged2)\n",
        "# #sklearn.impute.SimpleImputer\n",
        "# d = secomMerged2.isnull().sum()\n",
        "# j = []\n",
        "# for i in d.keys():\n",
        "#     if(d[i] >900):\n",
        "#         print(i, d[i])\n",
        "#         j.append(i)\n",
        "# secomMerged2.drop(j, inplace = True)\n",
        "# secomMerged2.replace(np.nan, 0, inplace = True)\n",
        "# #from sklearn.preprocessing import Imputer\n",
        "# #imputer = Imputer(missing_values = np.nan, strategy = 'mean')\n",
        "# secomMerged2.isnull().any().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvpk72wRMTg_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "f773aabb-885f-452b-dfae-bde3e3d2e444"
      },
      "source": [
        "secomMerged.head(3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passFail</th>\n",
              "      <th>datetime</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>550</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "      <th>561</th>\n",
              "      <th>562</th>\n",
              "      <th>563</th>\n",
              "      <th>564</th>\n",
              "      <th>565</th>\n",
              "      <th>566</th>\n",
              "      <th>567</th>\n",
              "      <th>568</th>\n",
              "      <th>569</th>\n",
              "      <th>570</th>\n",
              "      <th>571</th>\n",
              "      <th>572</th>\n",
              "      <th>573</th>\n",
              "      <th>574</th>\n",
              "      <th>575</th>\n",
              "      <th>576</th>\n",
              "      <th>577</th>\n",
              "      <th>578</th>\n",
              "      <th>579</th>\n",
              "      <th>580</th>\n",
              "      <th>581</th>\n",
              "      <th>582</th>\n",
              "      <th>583</th>\n",
              "      <th>584</th>\n",
              "      <th>585</th>\n",
              "      <th>586</th>\n",
              "      <th>587</th>\n",
              "      <th>588</th>\n",
              "      <th>589</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2008-07-19 11:55:00</td>\n",
              "      <td>3030.93</td>\n",
              "      <td>2564.00</td>\n",
              "      <td>2187.7333</td>\n",
              "      <td>1411.1265</td>\n",
              "      <td>1.3602</td>\n",
              "      <td>100.0</td>\n",
              "      <td>97.6133</td>\n",
              "      <td>0.1242</td>\n",
              "      <td>1.5005</td>\n",
              "      <td>0.0162</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>0.9455</td>\n",
              "      <td>202.4396</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.9558</td>\n",
              "      <td>414.8710</td>\n",
              "      <td>10.0433</td>\n",
              "      <td>0.9680</td>\n",
              "      <td>192.3963</td>\n",
              "      <td>12.5190</td>\n",
              "      <td>1.4026</td>\n",
              "      <td>-5419.00</td>\n",
              "      <td>2916.50</td>\n",
              "      <td>-4043.75</td>\n",
              "      <td>751.00</td>\n",
              "      <td>0.8955</td>\n",
              "      <td>1.7730</td>\n",
              "      <td>3.0490</td>\n",
              "      <td>64.2333</td>\n",
              "      <td>2.0222</td>\n",
              "      <td>0.1632</td>\n",
              "      <td>3.5191</td>\n",
              "      <td>83.3971</td>\n",
              "      <td>9.5126</td>\n",
              "      <td>50.6170</td>\n",
              "      <td>64.2588</td>\n",
              "      <td>49.3830</td>\n",
              "      <td>66.3141</td>\n",
              "      <td>...</td>\n",
              "      <td>12.93</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.1827</td>\n",
              "      <td>5.7349</td>\n",
              "      <td>0.3363</td>\n",
              "      <td>39.8842</td>\n",
              "      <td>3.2687</td>\n",
              "      <td>1.0297</td>\n",
              "      <td>1.0344</td>\n",
              "      <td>0.4385</td>\n",
              "      <td>0.1039</td>\n",
              "      <td>42.3877</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>533.8500</td>\n",
              "      <td>2.1113</td>\n",
              "      <td>8.95</td>\n",
              "      <td>0.3157</td>\n",
              "      <td>3.0624</td>\n",
              "      <td>0.1026</td>\n",
              "      <td>1.6765</td>\n",
              "      <td>14.9509</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.5005</td>\n",
              "      <td>0.0118</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>2.3630</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2008-07-19 12:32:00</td>\n",
              "      <td>3095.78</td>\n",
              "      <td>2465.14</td>\n",
              "      <td>2230.4222</td>\n",
              "      <td>1463.6606</td>\n",
              "      <td>0.8294</td>\n",
              "      <td>100.0</td>\n",
              "      <td>102.3433</td>\n",
              "      <td>0.1247</td>\n",
              "      <td>1.4966</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>-0.0148</td>\n",
              "      <td>0.9627</td>\n",
              "      <td>200.5470</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.1548</td>\n",
              "      <td>414.7347</td>\n",
              "      <td>9.2599</td>\n",
              "      <td>0.9701</td>\n",
              "      <td>191.2872</td>\n",
              "      <td>12.4608</td>\n",
              "      <td>1.3825</td>\n",
              "      <td>-5441.50</td>\n",
              "      <td>2604.25</td>\n",
              "      <td>-3498.75</td>\n",
              "      <td>-1640.25</td>\n",
              "      <td>1.2973</td>\n",
              "      <td>2.0143</td>\n",
              "      <td>7.3900</td>\n",
              "      <td>68.4222</td>\n",
              "      <td>2.2667</td>\n",
              "      <td>0.2102</td>\n",
              "      <td>3.4171</td>\n",
              "      <td>84.9052</td>\n",
              "      <td>9.7997</td>\n",
              "      <td>50.6596</td>\n",
              "      <td>64.2828</td>\n",
              "      <td>49.3404</td>\n",
              "      <td>64.9193</td>\n",
              "      <td>...</td>\n",
              "      <td>16.00</td>\n",
              "      <td>1.33</td>\n",
              "      <td>0.2829</td>\n",
              "      <td>7.1196</td>\n",
              "      <td>0.4989</td>\n",
              "      <td>53.1836</td>\n",
              "      <td>3.9139</td>\n",
              "      <td>1.7819</td>\n",
              "      <td>0.9634</td>\n",
              "      <td>0.1745</td>\n",
              "      <td>0.0375</td>\n",
              "      <td>18.1087</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>535.0164</td>\n",
              "      <td>2.4335</td>\n",
              "      <td>5.92</td>\n",
              "      <td>0.2653</td>\n",
              "      <td>2.0111</td>\n",
              "      <td>0.0772</td>\n",
              "      <td>1.1065</td>\n",
              "      <td>10.9003</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>208.2045</td>\n",
              "      <td>0.5019</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>4.4447</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>208.2045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2008-07-19 13:17:00</td>\n",
              "      <td>2932.61</td>\n",
              "      <td>2559.94</td>\n",
              "      <td>2186.4111</td>\n",
              "      <td>1698.0172</td>\n",
              "      <td>1.5102</td>\n",
              "      <td>100.0</td>\n",
              "      <td>95.4878</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>1.4436</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.9615</td>\n",
              "      <td>202.0179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.5157</td>\n",
              "      <td>416.7075</td>\n",
              "      <td>9.3144</td>\n",
              "      <td>0.9674</td>\n",
              "      <td>192.7035</td>\n",
              "      <td>12.5404</td>\n",
              "      <td>1.4123</td>\n",
              "      <td>-5447.75</td>\n",
              "      <td>2701.75</td>\n",
              "      <td>-4047.00</td>\n",
              "      <td>-1916.50</td>\n",
              "      <td>1.3122</td>\n",
              "      <td>2.0295</td>\n",
              "      <td>7.5788</td>\n",
              "      <td>67.1333</td>\n",
              "      <td>2.3333</td>\n",
              "      <td>0.1734</td>\n",
              "      <td>3.5986</td>\n",
              "      <td>84.7569</td>\n",
              "      <td>8.6590</td>\n",
              "      <td>50.1530</td>\n",
              "      <td>64.1114</td>\n",
              "      <td>49.8470</td>\n",
              "      <td>65.8389</td>\n",
              "      <td>...</td>\n",
              "      <td>16.16</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.0857</td>\n",
              "      <td>7.1619</td>\n",
              "      <td>0.3752</td>\n",
              "      <td>23.0713</td>\n",
              "      <td>3.9306</td>\n",
              "      <td>1.1386</td>\n",
              "      <td>1.5021</td>\n",
              "      <td>0.3718</td>\n",
              "      <td>0.1233</td>\n",
              "      <td>24.7524</td>\n",
              "      <td>267.064</td>\n",
              "      <td>0.9032</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.6219</td>\n",
              "      <td>0.4122</td>\n",
              "      <td>0.2562</td>\n",
              "      <td>0.4119</td>\n",
              "      <td>68.8489</td>\n",
              "      <td>535.0245</td>\n",
              "      <td>2.0293</td>\n",
              "      <td>11.21</td>\n",
              "      <td>0.1882</td>\n",
              "      <td>4.0923</td>\n",
              "      <td>0.0640</td>\n",
              "      <td>2.0952</td>\n",
              "      <td>9.2721</td>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.0484</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>82.8602</td>\n",
              "      <td>0.4958</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>3.1745</td>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.0484</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>82.8602</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows Ã— 592 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   passFail            datetime        0  ...     587     588       589\n",
              "0         0 2008-07-19 11:55:00  3030.93  ...  0.0000  0.0000    0.0000\n",
              "1         0 2008-07-19 12:32:00  3095.78  ...  0.0201  0.0060  208.2045\n",
              "2         1 2008-07-19 13:17:00  2932.61  ...  0.0484  0.0148   82.8602\n",
              "\n",
              "[3 rows x 592 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5yPdjDzMTK3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mioy8c60PHSG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75XibJrLO6ql"
      },
      "source": [
        "#secomMerged.to_excel(r'secomMerged.xlsx', index = False)\n",
        "#This cell no longer helpful. Have to resolve NaNs here. \n",
        "# secomMerged = pd.read_excel(r'secomMerged.xlsx',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJCvtnvuPn3o"
      },
      "source": [
        "# Creating X and Y. Splitting data into training, testing, and validation sets.\n",
        "\n",
        "First we create X and Y. For X we dropped the "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHsNsw-5O8NN"
      },
      "source": [
        "# Creating X and Y before\n",
        "# splitting data into training, testing, and validation sets.\n",
        "X = secomMerged.drop(columns = ['passFail', 'datetime'])\n",
        "X = X.astype('float32')\n",
        "Y = secomMerged['passFail']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Za6ajkfJGMj"
      },
      "source": [
        "# # Normalize all the columns in X to be between 0 and 1\n",
        "# def norm(column):\n",
        "#   return (column - min(column)) / (max(column) - min(column))\n",
        "\n",
        "# # normalize all colunns in X\n",
        "# for c in X.columns:\n",
        "#   X[c] = norm(X[c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFtg2UykJho7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "fa1869ed-5671-406a-9d8e-4fa4873a3241"
      },
      "source": [
        "#X.columns.values\n",
        "Y.hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2e402e32e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywND6kXQJuDs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "901cdd61-217a-476f-ff9a-966822ced0b6"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.hist(X[1])\n",
        "plt.show()\n",
        "#n = norm(X[1])\n",
        "#plt.hist(n)\n",
        "#plt.show()        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCz-2kdnO8jF",
        "outputId": "85ec3b5d-9f32-411d-b11d-cf9026ab6804"
      },
      "source": [
        "# Test-validation-train split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=37)\n",
        "\n",
        "# Split test into 50-50 test/validation\n",
        "#X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
        "#print('X valid stape: {}'.format(X_valid.shape))\n",
        "\n",
        "print('X train shape: {}'.format(X_train.shape))\n",
        "print('X test shape: {}'.format(X_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X train shape: (1253, 590)\n",
            "X test shape: (314, 590)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa3jpYOulS_-"
      },
      "source": [
        "We move on to construct a simple feed forward neural network. This code was adapted from:\n",
        "\n",
        "https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR-UUjEtTXO_",
        "outputId": "cc50820d-2b73-442f-86d0-00b9fd94cce1"
      },
      "source": [
        "#model.summary()\n",
        "# f-string printing, whatever is in {} is run as regular python code\n",
        "# the part :0.2f  means print out a decimal number (f==floating point number) that has 2 decimal places\n",
        "print(f'Percent that are defects: {100* sum(y_train>0)/len(y_train):0.2f}') # percent defect (y_train == +1)\n",
        "print(f'Total number in dataset: {len(y_train)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent that are defects: 6.62\n",
            "Total number in dataset: 1253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrdUkZdjM1Zo",
        "outputId": "55ffbbfe-eef2-408f-8b31-4469351cb543"
      },
      "source": [
        "# double check count of NaN.  There should be none.\n",
        "X_train.isna().sum().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4rM7jO4VXqn",
        "outputId": "f2c2a285-dcee-4f32-dbe4-cacbde4a09cd"
      },
      "source": [
        "# This is a very unbalanced data;  machine learning works best when balanced \n",
        "# i.e., closer to 50-50 of each class in 2 class problem\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "s = SMOTE(random_state=47)\n",
        "X_balanced, y_balanced = s.fit_sample(X_train, y_train)\n",
        "print(f'Percent that are defects: {100* sum(y_balanced>0)/len(y_balanced):0.2f}') \n",
        "print(f'Total number in dataset: {len(y_balanced)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent that are defects: 50.00\n",
            "Total number in dataset: 2340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPimYSGNlHCK"
      },
      "source": [
        "#################################################\n",
        "## Using TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZVNo6RDlGqV"
      },
      "source": [
        "# Our first neural network\n",
        "# Standard build feed forward neural network\n",
        "# 1 hidden layer, but we could add more.\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Flatten()) # input layer;  flattens the input layer to a 1 dimensional vector\n",
        "# Use dense layers == all nodes are connected to all nodes in the next layer \n",
        "model.add(keras.layers.Dense(10, activation=tf.nn.relu))  # hidden layer\n",
        "model.add(keras.layers.Dense(2, activation=tf.nn.relu))    # output layer, 2 output: defective or not defective\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTl4KOUGlIzD",
        "outputId": "f440a8d6-9095-4b78-aaa7-c65e2f6571c3"
      },
      "source": [
        "# train the model with the training set\n",
        "model.fit(X_balanced, y_balanced, batch_size=128, epochs=5, validation_split=0.2 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.1090 - accuracy: 0.5769 - val_loss: 0.7472 - val_accuracy: 0.2756\n",
            "Epoch 2/5\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3490 - accuracy: 0.5641 - val_loss: 0.6857 - val_accuracy: 0.3141\n",
            "Epoch 3/5\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.2838 - accuracy: 0.5646 - val_loss: 0.6857 - val_accuracy: 0.3355\n",
            "Epoch 4/5\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.2756 - accuracy: 0.5588 - val_loss: 0.6857 - val_accuracy: 0.3440\n",
            "Epoch 5/5\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.0289 - accuracy: 0.5641 - val_loss: 0.6931 - val_accuracy: 0.2692\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2e41372f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q10DhgkK9fNS"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX2cBOVw9gZw"
      },
      "source": [
        "This model's accuracy is inconsistant. There are several instances where our model has terrible accuracy during training, but manages to get higher accuracy on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwZf5DZhYKMi",
        "outputId": "a6f2349a-6746-4fac-f861-20750a3e7724"
      },
      "source": [
        "# check the accuracy of the test set\n",
        "print(\"Evaluate the test data set.\")\n",
        "results = model.evaluate(X_test, y_test, batch_size=128 )\n",
        "print(f'test loss (binary_crossentropy): {results[0]}')\n",
        "print(f'test accuracy:                   {results[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate the test data set.\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.7423 - accuracy: 0.7134\n",
            "test loss (binary_crossentropy): 0.7422713041305542\n",
            "test accuracy:                   0.7133758068084717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0kclaTid8AP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bdf81e8-fc09-4e33-d97f-8cfa20074b1d"
      },
      "source": [
        "r = model.predict_classes(X_test)\n",
        "#r"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-64-581d8e1939b5>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITwLok_8hLpa"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzwnuvLqhM19"
      },
      "source": [
        "- top-left: true positives\n",
        "- bottom-left: false positives\n",
        "-top-right: false-negatives\n",
        "bottom-right: true- negatives\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWHylLnXgbQq",
        "outputId": "48a4ef1e-be2d-4ac2-f7ff-83ed96fc5b2b"
      },
      "source": [
        "confusion_mat = tf.math.confusion_matrix(labels=y_test, predictions=r)\n",
        "print(confusion_mat)\n",
        "confusion_mat.numpy().max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[219  74]\n",
            " [ 16   5]], shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "219"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAgQJ7qdGsZ-"
      },
      "source": [
        "Next we plot our confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "xDA3feDbhrls",
        "outputId": "9423e7a8-77f0-40bf-99a0-1d3ed39fbaaf"
      },
      "source": [
        "import itertools\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "image = plot_confusion_matrix(confusion_mat.numpy(), classes=['0', '1'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[219  74]\n",
            " [ 16   5]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhsPsM3xlIWi",
        "outputId": "13a2742c-076f-4582-94e4-82abc554b373"
      },
      "source": [
        "np.isnan(X_balanced).sum()\n",
        "print(sum(sum(X_balanced==0.0)))\n",
        "X_balanced.shape[0] * X_balanced.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "346682\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1380600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EkL8zH-Bfzd"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b_GpruUBgxJ"
      },
      "source": [
        "Above is our confusion matrix for the feed forward nerual network. We can see that the greatest share of samples are true-positives (i.e. predicted label 0, is true label 0). The number of class 1 diapers (i.e. defective) is much smaller, since the instance of faulty diapers is relatively rare-only 21 out of the 314 values in the decision tree. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3We95lCTFkRY"
      },
      "source": [
        "The following section is adapted from: \n",
        "\n",
        "1.   https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/neural_network.py\n",
        "1.   https://caseythayerdotcom.files.wordpress.com/2019/09/predicting-manufacturing-failures-with-dense-and-recurrent-neural-networks.pdf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAbW1KrjP6V8"
      },
      "source": [
        "# 2nd Model:  Dense neural network\n",
        "# Standard build feed forward neural network with many Dense hidden layers\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Flatten()) # input layer;  flattens the input layer to a 1 dimensional vector\n",
        "# Use dense layers == all nodes are connected to all nodes in the next layer \n",
        "model.add(keras.layers.Dense(64, activation=tf.nn.relu))  # hidden layer 1\n",
        "model.add(keras.layers.Dense(64, activation=tf.nn.relu))  # hidden layer 2\n",
        "model.add(keras.layers.Dense(16, activation=tf.nn.relu))  # hidden layer 3\n",
        "model.add(keras.layers.Dense(2, activation=tf.nn.relu))   # output layer, 2 output: defective or not defective\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_hzEW5lP7zE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b296664-12ab-49a5-bd02-b3510ea4707e"
      },
      "source": [
        "# train the model\n",
        "model.fit(X_balanced, y_balanced, batch_size=64, epochs=2, validation_split=0.2 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 1.5097 - accuracy: 0.6047 - val_loss: 0.6931 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/2\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.6250 - val_loss: 0.6931 - val_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2e2b950c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmV4izEuE9fv",
        "outputId": "30b146c9-fe74-432d-c236-9dabefa3a4fc"
      },
      "source": [
        "# check the accuracy of the test set\n",
        "print(\"Evaluate the test data set.\")\n",
        "results = model.evaluate(X_test, y_test, batch_size=128 )\n",
        "print(f'test loss (binary_crossentropy): {results[0]}')\n",
        "print(f'test accuracy:                   {results[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate the test data set.\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.9331\n",
            "test loss (binary_crossentropy): 0.6931471824645996\n",
            "test accuracy:                   0.9331210255622864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "67TErbUCFHng",
        "outputId": "2b6df5d5-a0ee-4c34-89b9-27b085e5745f"
      },
      "source": [
        "r = model.predict_classes(X_test)\n",
        "confusion_mat = tf.math.confusion_matrix(labels=y_test, predictions=r)\n",
        "image = plot_confusion_matrix(confusion_mat.numpy(), classes=['0', '1'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[293   0]\n",
            " [ 21   0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEDjIhhMFHaG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ48u-ZwGM5g"
      },
      "source": [
        "Once again, a very unreliable model. It seems to classify everything as one type and its accuracy scores swing drastically.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFqjAhujFHDU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSnIfciXFG_h"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHe32KrZFG0-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMnbg95GP8WG"
      },
      "source": [
        "# our second neural network\n",
        "input_size = len(X.columns)\n",
        "batch_size = 32\n",
        "embedding_dim = 64\n",
        "rnn_units = 64\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(input_size, embedding_dim, \n",
        "                                 batch_input_shape=[batch_size, None])) # input layer;  \n",
        "#model.add(keras.layers.Flatten()) # input layer;  flattens the input layer to a 1 dimensional vector\n",
        "# LSTM is a type of recurrent neural network layer\n",
        "#model.add(keras.layers.LSTM(64))    #hidden layer\n",
        "# different type of RNN layer\n",
        "model.add(keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, \n",
        "                           recurrent_initializer='glorot_uniform'))\n",
        "# Use dense layers == all nodes are connected to all nodes in the next layer \n",
        "model.add(keras.layers.Dense(64, activation=tf.nn.relu))  # hidden layer\n",
        "#model.add(keras.layers.Dense(2, activation=tf.nn.softmax))    # output layer, 2 output: defective or not defective\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuAwmX92LbTl"
      },
      "source": [
        "# reshape the resampled training features to feed into RNN \n",
        "X_balanced = X_balanced.reshape((X_balanced.shape[0], 1, X_balanced.shape[1]))\n",
        "\n",
        "# convert test features from a dataframe to an array\n",
        "X_test = X_test.to_numpy()\n",
        "\n",
        "# reshape the test features to feed into RNN\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "aZkzOvjEWgQS",
        "outputId": "fb79e0fc-9599-49fd-d647-9f2d839cfc6b"
      },
      "source": [
        "# model = keras.Sequential()\n",
        "# model.add(keras.layers.Embedding(input_dim=input_size, output_dim=64))\n",
        "# # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
        "# model.add(keras.layers.GRU(256, return_sequences=True))\n",
        "# # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
        "# model.add(keras.layers.SimpleRNN(128))\n",
        "# model.add(keras.layers.Dense(2))\n",
        "\n",
        "# model.compile(optimizer='sgd', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# #print(model.summary())\n",
        "\n",
        "model = keras.models.Sequential() \n",
        "model.add(keras.layers.SimpleRNN(128, input_shape=(X_balanced.shape[1], X_balanced.shape[2]), ))                          \n",
        "model.add(keras.layers.LSTM(4))\n",
        "model.add(keras.layers.Dense(1, activation='relu'))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "# compile RNN model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-ca9845864b48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_balanced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_balanced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 926\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1090\u001b[0m       \u001b[0;31m# TODO(reedwm): We should assert input compatibility after the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m       \u001b[0;31m# are casted, not before.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m       \u001b[0minput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m       \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Use `self._name_scope()` to avoid auto-incrementing the name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 128]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuAQRl80TtQG",
        "outputId": "cf119a92-dad1-4223-8d78-6625790dca91"
      },
      "source": [
        "#model.summary()\n",
        "X_balanced.shape\n",
        "#np.isnan(X_balanced).sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2340, 590)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg3KUhGUP87M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4734a38b-2742-451d-ef50-a88f257459f1"
      },
      "source": [
        "# train the model with the training set\n",
        "model.fit(X_balanced, y_balanced, batch_size=batch_size, epochs=5, validation_split=0.2 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None) for input Tensor(\"embedding_2_input:0\", shape=(None, None), dtype=float32), but it was called on an input with incompatible shape (None, 1, 590).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-56183f5224a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the model with the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:747 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent.py:663 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:180 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer gru_2 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 1, 590, 64]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aTecq0slSka",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "outputId": "06f18caa-bb03-42b5-e08d-97058426413d"
      },
      "source": [
        "# check the accuracy of the test set\n",
        "\n",
        "print(\"Evaluate the test data set.\")\n",
        "results = model.evaluate(X_test, y_test, batch_size=128 )\n",
        "print(f'test loss (binary_crossentropy): {results[0]}')\n",
        "print(f'test accuracy:                   {results[1]}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate the test data set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-8e25a9558d96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluate the test data set.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'test loss (binary_crossentropy): {results[0]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'test accuracy:                   {results[1]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TraceContext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    844\u001b[0m               *args, **kwds)\n\u001b[1;32m    845\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  indices[112,0] = 3000 is not in [0, 590)\n\t [[node sequential_5/embedding_1/embedding_lookup (defined at <ipython-input-89-8e25a9558d96>:4) ]]\n  (1) Invalid argument:  indices[112,0] = 3000 is not in [0, 590)\n\t [[node sequential_5/embedding_1/embedding_lookup (defined at <ipython-input-89-8e25a9558d96>:4) ]]\n\t [[sequential_5/embedding_1/embedding_lookup/_8]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_test_function_10873]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_5/embedding_1/embedding_lookup:\n sequential_5/embedding_1/embedding_lookup/10309 (defined at /usr/lib/python3.6/contextlib.py:81)\n\nInput Source operations connected to node sequential_5/embedding_1/embedding_lookup:\n sequential_5/embedding_1/embedding_lookup/10309 (defined at /usr/lib/python3.6/contextlib.py:81)\n\nFunction call stack:\ntest_function -> test_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDCTd0hrP5t9"
      },
      "source": [
        "confusion_mat = tf.math.confusion_matrix(labels=y_test, predictions=r)\n",
        "image = plot_confusion_matrix(confusion_mat.numpy(), classes=['0', '1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Ld8vWOO85s"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58xBXGq53_bN"
      },
      "source": [
        "# 5\n",
        "\n",
        "Building these neural networks was an overall difficult task.\n",
        "\n",
        "We successfully built a feed forward neural network and a dense neural network, although with inconsistent results. Sadly, we couldn't get our RNN to successfully run in the end. One thing that makes the RNNs more difficult, is the recurrent features that filters data back to itself. Also, the shape of the data presents more issues as it flows through the recurrent neural network than simpler neural networks that don't feed back on themselves in that way. \n",
        "\n",
        "### Things we could've done differently\n",
        "- Spend more time with feature engineering, for example feature selection where we only use feature columns that were highly correlated to the target variable. Also, we could've scaled the numbers in the feature columns, e.g. linear scaling, log scaling, etc. We didn't do any of this. \n",
        "- Cleaning the data. All NaNs converted to zero. Would more thorough understanding of the features we could've selected more effective methods for dealing with those. For example convert to mean instead or entirely eliminate columns with too many NaNs in them.\n",
        "\n",
        "Making a recommendation to the diaper manufacturer is a difficult task, especially when using semiconductor data. Perhaps for subsequent projects we could perform Principal Component Analysis to parce out which features to include in our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGJs-ib55ZOM"
      },
      "source": [
        "\"Disposable Diaper.\" madehow.com. Retrieved Sept. 2020, from http://www.madehow.com/Volume-3/Disposable-Diaper.html#google_vignette"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr7crVCcPfkL"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W0VBw_aUYeK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}