{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgbPBI8vwZE2vV9KFp3UqP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inconw/ProjectPortfolio/blob/main/Lesson10_IngridConway.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTnMtrl1hRLJ"
      },
      "source": [
        "# UW PCE DS 420- Homework 10\n",
        "## Ingrid Conway\n",
        "### November 30, 2020\n",
        "\n",
        "In this homework, we take a sample of the IMDB dataset provided in TensorFlow Datasets to predict positive (1) or negative (0) sentiment. The data comes from http://ai.stanford.edu/~amaas/data/sentiment/ .\n",
        "\n",
        "We start by creating a benchmark model. The benchmark model we create counts the number of positive words and subtracting from that the number of negative words in each review. This number is the score. If the score is negative, it has more negative words than positive word and is subsequently classified as negative.\n",
        "\n",
        "We found that the accuracy of the model for predictions on our test set\n",
        "\n",
        "We get the positive and negative words from the NLTK library.\n",
        "\n",
        "In fact, a nice RNN is made here in the TensorFlow Tutorials:\n",
        "https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
        "\n",
        "We modify this to align with the Lesson 10 lab to use a simple RNN over a pandas sample.\n",
        "\n",
        "We will have to implement a basic cleaning, tokenizing, and embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGjkds1mhndY"
      },
      "source": [
        "#!pip install tensorflow_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9Fky8aoj50_",
        "outputId": "66757819-5d08-44d9-8ea5-e7bd17ba3d43"
      },
      "source": [
        "# Load Libraries\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoMyuPjLhuWo",
        "outputId": "99643b70-f05c-489d-82e0-10af383067e4"
      },
      "source": [
        "# enable GPU for neural network training speedup\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "print(device_name)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoYzudNShu6D"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLnQwxqZkWfI"
      },
      "source": [
        "### Here we load the data.\n",
        "\n",
        "The data comes in a nice format for setting up a test-preprocessing pipeline of data in the TF Dataset type.\n",
        "\n",
        "But we want to try to use Pandas, so we'll convert a portion of it into a dataframe.\n",
        "\n",
        "First, we load the data into our tf.dataset() objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wqizqcGj4yz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kBipPeNhvW8"
      },
      "source": [
        "# Setup the IMDB review data\n",
        "# The IMDB review data contains 25,000 reviews in the train set. And also 25,000 reviews in the test set.\n",
        "\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu9E1mWsiHfW"
      },
      "source": [
        "### Convert / Sample into Pandas.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "kyCGrXFIiAQD",
        "outputId": "521f3eaf-d7b9-4cc0-92f4-50caebcab6a4"
      },
      "source": [
        "# We can take up to 25,000 of each. But we'll take less for calculations to be easier.\n",
        "train_count = 10000\n",
        "test_count = 2000\n",
        "\n",
        "# Setup Train DF\n",
        "train_df = pd.DataFrame(columns=['review', 'label'])\n",
        "for review, label in train_dataset.take(train_count):\n",
        "    train_df = train_df.append({'review': review.numpy(), 'label': label.numpy()}, ignore_index=True)\n",
        "\n",
        "# Setup Test DF\n",
        "test_df = pd.DataFrame(columns=['review', 'label'])\n",
        "for review, label in test_dataset.take(test_count):\n",
        "    test_df = test_df.append({'review': review.numpy(), 'label': label.numpy()}, ignore_index=True)\n",
        "\n",
        "# Change to strings from the 'b' - binary columns\n",
        "train_df['review'] = train_df['review'].str.decode(\"utf-8\")\n",
        "test_df['review'] = test_df['review'].str.decode(\"utf-8\")\n",
        "    \n",
        "# Check size and examples\n",
        "print('Train shape: {}'.format(train_df.shape))\n",
        "print('Test shape: {}'.format(test_df.shape))\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (10000, 2)\n",
            "Test shape: (2000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This was an absolutely terrible movie. Don't b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I have been known to fall asleep during films,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mann photographs the Alberta Rocky Mountains i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This is the kind of film for a snowy Sunday af...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>As others have mentioned, all the women that g...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review label\n",
              "0  This was an absolutely terrible movie. Don't b...     0\n",
              "1  I have been known to fall asleep during films,...     0\n",
              "2  Mann photographs the Alberta Rocky Mountains i...     0\n",
              "3  This is the kind of film for a snowy Sunday af...     1\n",
              "4  As others have mentioned, all the women that g...     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "D4wM48QNCzsS",
        "outputId": "d59a7672-d299-4bc2-bfcf-787991d38ba2"
      },
      "source": [
        "train_df['review'][0]   # label of 0 means negative review sentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShdQokRQiNps"
      },
      "source": [
        "### Check for target imbalance problems.\n",
        "\n",
        "Next we check for the balance/ makeup of the target values to see if there exists any imbalance. There is a roughly 50-50 split between positive reviews and negative ones, so we don't need to worry about class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMeDoxR_iAoL",
        "outputId": "180d835f-05bd-48ac-9c4c-f5aee29508e9"
      },
      "source": [
        "# Check target imbalance? \n",
        "print(train_df['label'].value_counts())\n",
        "print(test_df['label'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    5002\n",
            "0    4998\n",
            "Name: label, dtype: int64\n",
            "1    1020\n",
            "0     980\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNBD_nv0iT4M"
      },
      "source": [
        "### Now we split the sets into train-validation-test\n",
        "\n",
        "The data is already split into train-test. To make a validation set, we'll just split the test set in half."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "D078-DNWiA9r",
        "outputId": "cd5e1480-22cc-4836-daca-c32911f7e785"
      },
      "source": [
        "# Split into train-validation-test by taking half of the test set into validation\n",
        "test_df, valid_df = train_test_split(test_df, test_size = 0.5)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "valid_df = valid_df.reset_index(drop=True)\n",
        "\n",
        "print('Test shape: {}'.format(test_df.shape))\n",
        "print('Valid shape: {}'.format(valid_df.shape))\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test shape: (1000, 2)\n",
            "Valid shape: (1000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This film held my interest from the beginning ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK, Chuck Norris has shown up in many an enter...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I found this movie to be quite enjoyable and f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The first time I saw this film I was a kid. I ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I really liked the idea of traveling between d...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review label\n",
              "0  This film held my interest from the beginning ...     1\n",
              "1  OK, Chuck Norris has shown up in many an enter...     0\n",
              "2  I found this movie to be quite enjoyable and f...     1\n",
              "3  The first time I saw this film I was a kid. I ...     1\n",
              "4  I really liked the idea of traveling between d...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIoTPK-Pieb8"
      },
      "source": [
        "### Clean the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ_8akYgidA_"
      },
      "source": [
        "# This was borrowed from the 'L10_Lab_NLP' class notebook.\n",
        "# Lowercase everything\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDZme2aMumbr",
        "outputId": "ce0de317-1577-4532-85e1-c16cac69090d"
      },
      "source": [
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbGQLl5n8Csj",
        "outputId": "7cefd199-e936-4540-e91b-73e58c12007a"
      },
      "source": [
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\")) \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# lemmatize converts all words to their base root word\n",
        "# E.g., 'buses' --> 'bus', the singular\n",
        "#       'singing' --> 'sing', the main root of the verb sing\n",
        "# Example of list of words lemmatized with 'v', for converting verbs to present tense:\n",
        "word_examples = ['was', 'sings', 'singing', 'sang', 'bus', 'buses', 'radii', 'radius']\n",
        "[lemmatizer.lemmatize(token, 'v') for token in word_examples]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['be', 'sing', 'sing', 'sing', 'bus', 'bus', 'radii', 'radius']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f7m28Wm73HR"
      },
      "source": [
        "def text_process(my_string, remove_stops=True):\n",
        "  # Lowercase (need to make sure it's a string because of garbage inputs...)\n",
        "  sentence = str(my_string).lower()\n",
        "  # Take non characters and replace with space.  This removes punctuation.\n",
        "  sentence = re.sub(r'[^\\w]', ' ', sentence)\n",
        "  # Remove numbers\n",
        "  sentence = re.sub(r'\\d', ' ', sentence)\n",
        "  # Remove extra white space\n",
        "  sentence = ' '.join(sentence.split())\n",
        "  # lemmatize converts words into their root words. E.g., all plural words into singular\n",
        "  sentence = [lemmatizer.lemmatize(token) for token in sentence.split(\" \")]\n",
        "  # POS='v' means lemmatize verbs to their root verb present tense\n",
        "  sentence = [lemmatizer.lemmatize(token, \"v\") for token in sentence]  \n",
        "  # remove stop words, which don't convey as much meaning\n",
        "  sentence = [word for word in sentence if not word in stop_words]\n",
        "  sentence = \" \".join(sentence)\n",
        "  filtered_w_list = []\n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Rlt2cYTLERor",
        "outputId": "2b5d409d-3b91-41c9-a98f-589e278715aa"
      },
      "source": [
        "# stop_words[0:20]\n",
        "text_process('''Hi there. you are in your offices             working \n",
        "\n",
        "on your chair.''')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hi office work chair'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd7LKiMSinBr"
      },
      "source": [
        "# 3\n",
        "### Create a rule-based benchmark (only need one)\n",
        "\n",
        "Ideas:\n",
        "\n",
        "- Load nltk's sentiment library.\n",
        "- Create your own sentiment library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-In4K9zXiC3c",
        "outputId": "2359d459-ea0a-43f2-ed43-b6a72e277950"
      },
      "source": [
        "from nltk.tokenize import treebank\n",
        "nltk.download('opinion_lexicon')\n",
        "\n",
        "from nltk.corpus import opinion_lexicon"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/opinion_lexicon.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pXl49O3HJD2",
        "outputId": "94b79f06-a9ab-4079-cc0d-75af8734c736"
      },
      "source": [
        "# Let's look at lexicon/dictionary\n",
        "pos_set = set(opinion_lexicon.positive())\n",
        "neg_set = set(opinion_lexicon.negative())\n",
        "\n",
        "print('{:,} positive words:'.format(len(list(pos_set))))\n",
        "print(list(pos_set)[:10])\n",
        "\n",
        "print('{:,} negative words:'.format(len(list(neg_set))))\n",
        "print(list(neg_set)[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2,006 positive words:\n",
            "['insightfully', 'feasible', 'whoooa', 'joy', 'distinguished', 'admiration', 'vibrant', 'rejoicingly', 'prodigious', 'eloquently']\n",
            "4,783 negative words:\n",
            "['louder', 'quarrellous', 'tarnishes', 'repulsively', 'injure', 'injustice', 'mistress', 'toll', 'corrode', 'abnormal']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrtIuajfHJjm"
      },
      "source": [
        "def sentiment_base(my_string):\n",
        "    # Get words\n",
        "    words = [w for w in word_tokenize(my_string)]\n",
        "    # Get + and - scores:\n",
        "    pos_score = sum([1 if w in pos_set else 0 for w in words])\n",
        "    neg_score = sum([1 if w in neg_set else 0 for w in words])\n",
        "    return pos_score - neg_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk18VWO6H0iG",
        "outputId": "7e60c5b3-7e29-4ff2-a9a5-079a7fed0233"
      },
      "source": [
        "# Run for one example sentence\n",
        "r = text_process(\"The movie was not bad.  It is the opposite of horrible.\")\n",
        "# our simple count of positive - negative words is not foolproof\n",
        "sentiment_base( r  )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrwiy4leIjoD",
        "outputId": "9cce29e2-3934-4459-f5eb-2b2bc7a1b0cf"
      },
      "source": [
        "# Run for the validation set\n",
        "def run_rule_sentiment(df):\n",
        "  '''Run the rule based setiment analysis based on the number of positive\n",
        "  words minuse the number of negative words.  Return the accuracy (what percent  it\n",
        "  correctly predicts)'''\n",
        "  rule_based = pd.DataFrame()\n",
        "  rule_based['score'] = df['review'].apply( text_process ).apply( sentiment_base )\n",
        "  rule_based['label'] = df['label']\n",
        "  def is_correct(r):\n",
        "    '''Returns True if the prediction is correct; False if the prediciton is wrong.\n",
        "    r is the row of a dataframe that has a 'score' and 'label' column'''\n",
        "    if r['score'] <= 0:\n",
        "      if r['label'] == 1:\n",
        "        return False\n",
        "      else:\n",
        "        return True\n",
        "    else: # i.e., r['score'] > 0\n",
        "      if r['label'] == 1: \n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "  rule_based['correct'] = rule_based.apply(is_correct, axis=1)\n",
        "  #rule_based.head(20)\n",
        "  accuracy = sum(rule_based['correct']) / len(rule_based)\n",
        "  return accuracy  \n",
        "accuracy_train      = run_rule_sentiment(train_df)\n",
        "accuracy_validation = run_rule_sentiment(valid_df)\n",
        "accuracy_test       = run_rule_sentiment(test_df)\n",
        "print(f'Accuracy of train set predictions:      {accuracy_train}')\n",
        "print(f'Accuracy of validation set predictions: {accuracy_validation}')\n",
        "print(f'Accuracy of test set predictions:       {accuracy_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of train set predictions:      0.7264\n",
            "Accuracy of validation set predictions: 0.728\n",
            "Accuracy of test set predictions:       0.744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKzeNjVmirv-"
      },
      "source": [
        "### Do some exploration of cleaned data.\n",
        "\n",
        "What should we pick for the max_vocab size? (Look at # of unique words in train set, how many words appear > 1 times? > 3 times?\n",
        "\n",
        "What should we pick for the max individual sequence length? (Look at # of total words in each entry).\n",
        "\n",
        "> Note: there isn't a perfect answer to these. Pick what you feel is appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMG0UuSqI8bg"
      },
      "source": [
        "# Count the number of times words are seen in ALL reviews in the training set\n",
        "# with a Counter dictionary data type\n",
        "from collections import Counter\n",
        "count = Counter()\n",
        "for r in train_df['review']:\n",
        "  r = text_process(r)\n",
        "  count.update(r.split(' '))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDdRXwNXKzf8",
        "outputId": "06472eef-6ece-408c-c988-349947ac9bd6"
      },
      "source": [
        "print(f'''The total number of different words after text_process() run for ALL\n",
        "moview reviews in the train_df is:   {len(count)}''')\n",
        "# Example of using this Counter dictionary.  How many times is a particular word seen?\n",
        "print(f'The number of times \"movie\" is seen in all reviews in training: {count[\"movie\"]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total number of different words after text_process() run for ALL\n",
            "moview reviews in the train_df is:   38841\n",
            "The number of times \"movie\" is seen in all reviews in training: 20658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG8IE1W_Ljlo",
        "outputId": "a3d11f5e-4c4f-48e2-bd88-d1073bd6bd2b"
      },
      "source": [
        "# loop through ALL words (w) and their counts of times seen (c)\n",
        "words_few = {}  # empty dict of words that appear only few times.  will fill this dict next\n",
        "for (w, c) in count.items():  \n",
        "  if c<=3:\n",
        "      words_few[w] = c\n",
        "print(f'The number of words that appear three or fewer times in the entire colleciton of reviews: {len(words_few)}')\n",
        "#list(count.items())[0:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of words that appear three or fewer times in the entire colleciton of reviews: 22632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VexLAlyHNyXF",
        "outputId": "6daef5a0-0ca9-41ee-f188-47f010ba6b55"
      },
      "source": [
        "list(words_few.items())[0:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('columbian', 1),\n",
              " ('conchita', 3),\n",
              " ('sette', 3),\n",
              " ('gunfighters', 2),\n",
              " ('microscopically', 2),\n",
              " ('gradation', 1),\n",
              " ('winningham', 3),\n",
              " ('oscillator', 1),\n",
              " ('vibrate', 3),\n",
              " ('herringbone', 1),\n",
              " ('emblazered', 1),\n",
              " ('ensweatered', 1),\n",
              " ('glamourous', 2),\n",
              " ('considerate', 3),\n",
              " ('pilcher', 2),\n",
              " ('polonia', 1),\n",
              " ('teacup', 2),\n",
              " ('nancherrow', 3),\n",
              " ('backlighting', 2),\n",
              " ('dewy', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG1fQSuyI7O8",
        "outputId": "e7bd505d-1019-4ba5-f937-1818b637a858"
      },
      "source": [
        "count.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('br', 40435),\n",
              " ('movie', 20658),\n",
              " ('wa', 19514),\n",
              " ('film', 19441),\n",
              " ('one', 11109),\n",
              " ('make', 9433),\n",
              " ('like', 8968),\n",
              " ('see', 8331),\n",
              " ('get', 7220),\n",
              " ('ha', 6679)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqDoIe6kOi5z"
      },
      "source": [
        "# What is the length of the number of words in reviews after do text_process()?\n",
        "train_df.head()\n",
        "#def get_count(x):\n",
        "#  return len( text_process( x ).split(' ') )\n",
        "\n",
        "len( text_process(\"Here is my review of the bad movie\").split(' ') )\n",
        "rev_len = pd.DataFrame()   # dataframe of review lengths (empty now)\n",
        "rev_len['NumWords'] = train_df['review'].apply(  \n",
        "    lambda x: len( text_process( x ).split(' ') )  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "rHcMka7BOiDJ",
        "outputId": "6c5045da-7611-4d1e-b9f3-0c0898c83174"
      },
      "source": [
        "rev_len.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NumWords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   NumWords\n",
              "0        69\n",
              "1        55\n",
              "2        78\n",
              "3        46\n",
              "4        38"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndSk5PUYqUsh"
      },
      "source": [
        "Below we make a histogram to visualize the number of words across reviews. We subsequently cap the max review length at 130 words. We fiddled with the max_vocab_size, but settled at 6000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "1U7hesofOg__",
        "outputId": "b7396182-28d7-4eca-ef59-0e04c194ceb1"
      },
      "source": [
        "rev_len['NumWords'].hist()\n",
        "rev_len.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NumWords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10000.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>125.69560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>94.64632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>9.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>67.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>94.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>153.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1457.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          NumWords\n",
              "count  10000.00000\n",
              "mean     125.69560\n",
              "std       94.64632\n",
              "min        9.00000\n",
              "25%       67.00000\n",
              "50%       94.00000\n",
              "75%      153.00000\n",
              "max     1457.00000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXVUlEQVR4nO3df4xdZZ3H8fdnqVBl3E4rOtttm21dGw3aiHQCJRhzh65tAUPZRElJs0zZbrrZZV3dJZGiYbsCJmVl/UFW0YnULS46dqssTUHJ7MjE8AeIFWz5Ye0IRTopVJlSdgB/VL/7x3kuXqYzvffSM3dueD6v5Oae85znnPs9p53POfPcc+8oIjAzszz80XQXYGZmrePQNzPLiEPfzCwjDn0zs4w49M3MMjJjugs4ntNOOy0WLlzY9HovvPACp556avkFlcx1lst1lst1lquVde7ateuXEfHmCRdGRNs+li5dGq/GPffc86rWazXXWS7XWS7XWa5W1gn8MCbJVQ/vmJllxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llpK2/huFELdx457S87v7NF07L65qZ1eMrfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwyUjf0Jb1d0kM1j+clfVTSHEkDkval59mpvyTdJGlY0m5JZ9Zsqzf13yepdyp3zMzMjlU39CNib0ScERFnAEuBF4HbgY3AYEQsBgbTPMD5wOL02ADcDCBpDrAJOBs4C9hUPVGYmVlrNDu8sxz4WUQ8CawGtqb2rcDFaXo1cGsU7gM6Jc0FVgIDETEaEYeBAWDVCe+BmZk1TBHReGdpC/CjiPgPSc9FRGdqF3A4Ijol7QQ2R8S9adkgcBVQAWZGxPWp/RrgpYi4cdxrbKD4DYGurq6l/f39Te/U2NgYHR0d7Bk50vS6ZVgyb1ZD/ap1tjvXWS7XWS7Xeayenp5dEdE90bKGv1pZ0snARcDV45dFREhq/OxxHBHRB/QBdHd3R6VSaXobQ0NDVCoV1k3XVyuvrTTUr1pnu3Od5XKd5XKdzWlmeOd8iqv8Z9L8M2nYhvR8KLWPAAtq1puf2iZrNzOzFmkm9C8FvlEzvwOo3oHTC9xR035ZuotnGXAkIg4CdwMrJM1Ob+CuSG1mZtYiDQ3vSDoVeD/wtzXNm4FtktYDTwKXpPa7gAuAYYo7fS4HiIhRSdcBD6R+10bE6AnvgZmZNayh0I+IF4A3jWt7luJunvF9A7hiku1sAbY0X6aZmZXBn8g1M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMNhb6kTknbJf1E0mOSzpE0R9KApH3peXbqK0k3SRqWtFvSmTXb6U3990nqnfwVzcxsKjR6pf954LsR8Q7g3cBjwEZgMCIWA4NpHuB8YHF6bABuBpA0B9gEnA2cBWyqnijMzKw16oa+pFnA+4BbACLiNxHxHLAa2Jq6bQUuTtOrgVujcB/QKWkusBIYiIjRiDgMDACrSt0bMzM7rkau9BcBvwC+KulBSV+RdCrQFREHU5+nga40PQ94qmb9A6ltsnYzM2sRRcTxO0jdwH3AuRFxv6TPA88DH46Izpp+hyNitqSdwOaIuDe1DwJXARVgZkRcn9qvAV6KiBvHvd4GimEhurq6lvb39ze9U2NjY3R0dLBn5EjT65ZhybxZDfWr1tnuXGe5XGe5XOexenp6dkVE90TLZjSw/gHgQETcn+a3U4zfPyNpbkQcTMM3h9LyEWBBzfrzU9sIRfDXtg+Nf7GI6AP6ALq7u6NSqYzvUtfQ0BCVSoV1G+9set0y7F9baahftc525zrL5TrL5TqbU3d4JyKeBp6S9PbUtBx4FNgBVO/A6QXuSNM7gMvSXTzLgCNpGOhuYIWk2ekN3BWpzczMWqSRK32ADwO3SToZeBy4nOKEsU3SeuBJ4JLU9y7gAmAYeDH1JSJGJV0HPJD6XRsRo6XshZmZNaSh0I+Ih4CJxoeWT9A3gCsm2c4WYEszBZqZWXn8iVwzs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLSEOhL2m/pD2SHpL0w9Q2R9KApH3peXZql6SbJA1L2i3pzJrt9Kb++yT1Ts0umZnZZJq50u+JiDMiovoH0jcCgxGxGBhM8wDnA4vTYwNwMxQnCWATcDZwFrCpeqIwM7PWOJHhndXA1jS9Fbi4pv3WKNwHdEqaC6wEBiJiNCIOAwPAqhN4fTMza5Iion4n6QngMBDAlyOiT9JzEdGZlgs4HBGdknYCmyPi3rRsELgKqAAzI+L61H4N8FJE3DjutTZQ/IZAV1fX0v7+/qZ3amxsjI6ODvaMHGl63TIsmTeroX7VOtud6yyX6yyX6zxWT0/PrppRmVeY0eA23hsRI5LeAgxI+kntwogISfXPHg2IiD6gD6C7uzsqlUrT2xgaGqJSqbBu451llNS0/WsrDfWr1tnuXGe5XGe5XGdzGhreiYiR9HwIuJ1iTP6ZNGxDej6Uuo8AC2pWn5/aJms3M7MWqRv6kk6V9MbqNLACeBjYAVTvwOkF7kjTO4DL0l08y4AjEXEQuBtYIWl2egN3RWozM7MWaWR4pwu4vRi2Zwbw9Yj4rqQHgG2S1gNPApek/ncBFwDDwIvA5QARMSrpOuCB1O/aiBgtbU/MzKyuuqEfEY8D756g/Vlg+QTtAVwxyba2AFuaL9PMzMrgT+SamWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlpGGQ1/SSZIelLQzzS+SdL+kYUnflHRyaj8lzQ+n5QtrtnF1at8raWXZO2NmZsfXzJX+R4DHauZvAD4bEW8DDgPrU/t64HBq/2zqh6TTgTXAO4FVwBclnXRi5ZuZWTMaCn1J84ELga+keQHnAdtTl63AxWl6dZonLV+e+q8G+iPi1xHxBDAMnFXGTpiZWWMavdL/HPAx4Pdp/k3AcxFxNM0fAOal6XnAUwBp+ZHU/+X2CdYxM7MWmFGvg6QPAIciYpekylQXJGkDsAGgq6uLoaGhprcxNjbG0NAQVy45Wr/zFGi05mqd7c51lst1lst1Nqdu6APnAhdJugCYCfwx8HmgU9KMdDU/HxhJ/UeABcABSTOAWcCzNe1Vteu8LCL6gD6A7u7uqFQqTe/U0NAQlUqFdRvvbHrdMuxfW2moX7XOduc6y+U6y+U6m1N3eCciro6I+RGxkOKN2O9FxFrgHuCDqVsvcEea3pHmScu/FxGR2teku3sWAYuBH5S2J2ZmVlcjV/qTuQrol3Q98CBwS2q/BfiapGFglOJEQUQ8Imkb8ChwFLgiIn53Aq9vZmZNair0I2IIGErTjzPB3TcR8SvgQ5Os/yngU80WaWZm5fAncs3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjdUNf0kxJP5D0Y0mPSPpkal8k6X5Jw5K+Kenk1H5Kmh9OyxfWbOvq1L5X0sqp2ikzM5tYI1f6vwbOi4h3A2cAqyQtA24APhsRbwMOA+tT//XA4dT+2dQPSacDa4B3AquAL0o6qcydMTOz46sb+lEYS7OvS48AzgO2p/atwMVpenWaJy1fLkmpvT8ifh0RTwDDwFml7IWZmTVEEVG/U3FFvgt4G/AF4NPAfelqHkkLgO9ExLskPQysiogDadnPgLOBf03r/FdqvyWts33ca20ANgB0dXUt7e/vb3qnxsbG6OjoYM/IkabXLcOSebMa6lets925znK5znK5zmP19PTsiojuiZbNaGQDEfE74AxJncDtwDtKrG/8a/UBfQDd3d1RqVSa3sbQ0BCVSoV1G+8subrG7F9baahftc525zrL5TrL5Tqb09TdOxHxHHAPcA7QKal60pgPjKTpEWABQFo+C3i2tn2CdczMrAUauXvnzekKH0mvB94PPEYR/h9M3XqBO9L0jjRPWv69KMaQdgBr0t09i4DFwA/K2hEzM6uvkeGducDWNK7/R8C2iNgp6VGgX9L1wIPALan/LcDXJA0DoxR37BARj0jaBjwKHAWuSMNGZmbWInVDPyJ2A++ZoP1xJrj7JiJ+BXxokm19CvhU82WamVkZ/IlcM7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy0jd0Je0QNI9kh6V9Iikj6T2OZIGJO1Lz7NTuyTdJGlY0m5JZ9Zsqzf13yepd+p2y8zMJtLIlf5R4MqIOB1YBlwh6XRgIzAYEYuBwTQPcD6wOD02ADdDcZIANgFnU/xB9U3VE4WZmbVG3dCPiIMR8aM0/X/AY8A8YDWwNXXbClycplcDt0bhPqBT0lxgJTAQEaMRcRgYAFaVujdmZnZciojGO0sLge8D7wJ+HhGdqV3A4YjolLQT2BwR96Zlg8BVQAWYGRHXp/ZrgJci4sZxr7GB4jcEurq6lvb39ze9U2NjY3R0dLBn5EjT65ZhybxZDfWr1tnuXGe5XGe5XOexenp6dkVE90TLZjS6EUkdwLeAj0bE80XOFyIiJDV+9jiOiOgD+gC6u7ujUqk0vY2hoSEqlQrrNt5ZRklN27+20lC/ap3tznWWy3WWy3U2p6G7dyS9jiLwb4uIb6fmZ9KwDen5UGofARbUrD4/tU3WbmZmLdLI3TsCbgEei4jP1CzaAVTvwOkF7qhpvyzdxbMMOBIRB4G7gRWSZqc3cFekNjMza5FGhnfOBf4K2CPpodT2cWAzsE3SeuBJ4JK07C7gAmAYeBG4HCAiRiVdBzyQ+l0bEaOl7EWbWdjgsNKVS46WOgS1f/OFpW3LzF6b6oZ+ekNWkyxePkH/AK6YZFtbgC3NFGhmZuXxJ3LNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy0gjfxh9i6RDkh6uaZsjaUDSvvQ8O7VL0k2ShiXtlnRmzTq9qf8+Sb0TvZaZmU2tRq70/xNYNa5tIzAYEYuBwTQPcD6wOD02ADdDcZIANgFnA2cBm6onCjMza526oR8R3wdGxzWvBram6a3AxTXtt0bhPqBT0lxgJTAQEaMRcRgY4NgTiZmZTTFFRP1O0kJgZ0S8K80/FxGdaVrA4YjolLQT2BwR96Zlg8BVQAWYGRHXp/ZrgJci4sYJXmsDxW8JdHV1Le3v7296p8bGxujo6GDPyJGm122lrtfDMy+Vt70l82aVt7Ea1ePZ7lxnuVxnuVpZZ09Pz66I6J5o2YwT3XhEhKT6Z47Gt9cH9AF0d3dHpVJpehtDQ0NUKhXWbbyzrLKmxJVLjvLve074n+Bl+9dWSttWrerxbHeus1yus1ztUuervXvnmTRsQ3o+lNpHgAU1/eantsnazcyshV5t6O8Aqnfg9AJ31LRflu7iWQYciYiDwN3ACkmz0xu4K1KbmZm1UN2xBUnfoBiTP03SAYq7cDYD2yStB54ELknd7wIuAIaBF4HLASJiVNJ1wAOp37URMf7NYTMzm2J1Qz8iLp1k0fIJ+gZwxSTb2QJsaao6MzMrlT+Ra2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWkfK+4tGm3cIp+lbRK5ccrfuNpfs3Xzglr21m5fKVvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhF/OMtKMVUfDKvHHwoza07Lr/QlrZK0V9KwpI2tfn0zs5y1NPQlnQR8ATgfOB24VNLprazBzCxnrb7SPwsYjojHI+I3QD+wusU1mJllq9Vj+vOAp2rmDwBn13aQtAHYkGbHJO19Fa9zGvDLV1VhC/2j6zxhuuEVs21b5zius1yu81h/NtmCtnsjNyL6gL4T2YakH0ZEd0klTRnXWS7XWS7XWa52qbPVwzsjwIKa+fmpzczMWqDVof8AsFjSIkknA2uAHS2uwcwsWy0d3omIo5L+AbgbOAnYEhGPTMFLndDwUAu5znK5znK5znK1RZ2KiOmuwczMWsRfw2BmlhGHvplZRl5Tod9OX/EgaYGkeyQ9KukRSR9J7XMkDUjal55np3ZJuinVvlvSmS2u9yRJD0rameYXSbo/1fPN9MY7kk5J88Np+cIW1tgpabukn0h6TNI57Xg8Jf1T+jd/WNI3JM1sh+MpaYukQ5Iermlr+vhJ6k3990nqbVGdn07/7rsl3S6ps2bZ1anOvZJW1rRPaR5MVGfNsislhaTT0vy0Hc9jRMRr4kHxxvDPgLcCJwM/Bk6fxnrmAmem6TcCP6X46ol/Azam9o3ADWn6AuA7gIBlwP0trvefga8DO9P8NmBNmv4S8Hdp+u+BL6XpNcA3W1jjVuBv0vTJQGe7HU+KDyA+Aby+5jiua4fjCbwPOBN4uKatqeMHzAEeT8+z0/TsFtS5ApiRpm+oqfP09LN+CrAoZcBJrciDiepM7QsoblZ5Ejhtuo/nMXW34gehFQ/gHODumvmrgaunu66aeu4A3g/sBeamtrnA3jT9ZeDSmv4v92tBbfOBQeA8YGf6j/nLmh+yl49t+s98TpqekfqpBTXOSmGqce1tdTz5w6fO56TjsxNY2S7HE1g4LkybOn7ApcCXa9pf0W+q6hy37C+B29L0K37Oq8ezVXkwUZ3AduDdwH7+EPrTejxrH6+l4Z2JvuJh3jTV8grpV/b3APcDXRFxMC16GuhK09NZ/+eAjwG/T/NvAp6LiKMT1PJynWn5kdR/qi0CfgF8NQ1DfUXSqbTZ8YyIEeBG4OfAQYrjs4v2O55VzR6/dvg5+2uKq2aOU8+01ClpNTASET8et6ht6nwthX5bktQBfAv4aEQ8X7ssilP7tN4zK+kDwKGI2DWddTRgBsWv0jdHxHuAFyiGI17WJsdzNsWXCC4C/hQ4FVg1nTU1qh2OXz2SPgEcBW6b7lrGk/QG4OPAv0x3LcfzWgr9tvuKB0mvowj82yLi26n5GUlz0/K5wKHUPl31nwtcJGk/xbeengd8HuiUVP3wXm0tL9eZls8Cnm1BnQeAAxFxf5rfTnESaLfj+RfAExHxi4j4LfBtimPcbsezqtnjN20/Z5LWAR8A1qYTFMepZzrq/HOKk/2P08/TfOBHkv6knep8LYV+W33FgyQBtwCPRcRnahbtAKrv0PdSjPVX2y9L7/IvA47U/No9ZSLi6oiYHxELKY7Z9yJiLXAP8MFJ6qzW/8HUf8qvDiPiaeApSW9PTcuBR2mz40kxrLNM0hvS/4FqnW11PGs0e/zuBlZImp1+q1mR2qaUpFUUQ5AXRcSL4+pfk+6CWgQsBn7ANORBROyJiLdExML083SA4maOp2mn4zmVbxi0+kHxDvlPKd61/8Q01/Jeil+VdwMPpccFFOO1g8A+4H+BOam/KP7AzM+APUD3NNRc4Q9377yV4odnGPhv4JTUPjPND6flb21hfWcAP0zH9H8o7nZou+MJfBL4CfAw8DWKO0um/XgC36B4n+G3FIG0/tUcP4ox9eH0uLxFdQ5TjH1Xf5a+VNP/E6nOvcD5Ne1TmgcT1Tlu+X7+8EbutB3P8Q9/DYOZWUZeS8M7ZmZWh0PfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4z8P6uBuqPY6BU2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxN7YQyUiDUD"
      },
      "source": [
        "# Setup arrays\n",
        "x_train = train_df['review'].apply(text_process).values\n",
        "y_train = train_df['label'].astype(int).values\n",
        "x_valid = valid_df['review'].apply(text_process).values\n",
        "y_valid = valid_df['label'].astype(int).values\n",
        "x_test = test_df['review'].apply(text_process).values\n",
        "y_test = test_df['label'].astype(int).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dopYxcU6DSs0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbBTQetSi1Vg",
        "outputId": "1ee458a2-0316-47a8-91a3-75a20c84e763"
      },
      "source": [
        "# Setup a vocab embedding\n",
        "max_vocab_size=6000 # Probably update this based on prior explorations\n",
        "max_sequence_len=130 # Probably update this based on prior explorations\n",
        "\n",
        "# Initialize our encoder\n",
        "k_tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<unk>')\n",
        "\n",
        "# Fit our tokenizer\n",
        "k_tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "# Convert to sequences\n",
        "x_train_seq = k_tokenizer.texts_to_sequences(x_train)\n",
        "x_valid_seq = k_tokenizer.texts_to_sequences(x_valid)\n",
        "x_test_seq = k_tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "print(x_train_seq[0])\n",
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 375, 306, 3, 3743, 1107, 2821, 389, 1, 34, 50, 128, 251, 158, 85, 396, 17, 34, 37, 45, 993, 3, 567, 604, 3, 321, 3479, 79, 2062, 248, 983, 24, 1, 2907, 7, 293, 2584, 1862, 1, 1, 254, 3744, 3187, 39, 1231, 2821, 4, 84, 983, 723, 4978, 3, 4, 3480, 69, 103, 350, 3, 8, 837, 50, 8, 1107, 2821, 13, 160, 45, 1021, 322]\n",
            "wa absolutely terrible movie lure christopher walken michael ironside great actor must simply worst role history even great act could redeem movie ridiculous storyline movie early ninety u propaganda piece pathetic scene columbian rebel make case revolution maria conchita alonso appear phony pseudo love affair walken wa nothing pathetic emotional plug movie wa devoid real mean disappoint movie like ruin actor like christopher walken good name could barely sit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivVAg0TBi1oH",
        "outputId": "a5a53f56-1b9c-415b-a882-303be7de593e"
      },
      "source": [
        "example_review = 'i love this movie so much snuffalufagus'\n",
        "\n",
        "# Test encoding-> Any unknown word should get mapped to '1'\n",
        "encoded_example = k_tokenizer.texts_to_sequences([example_review])\n",
        "encoded_example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 39, 1, 3, 1, 27, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgj194gqi17X"
      },
      "source": [
        "# Add padding after sequences or cutoff to limit length.\n",
        "# Padding can be before or after sequence. We want to pad w/ zeros after.\n",
        "x_train_padded = pad_sequences(x_train_seq, padding='post', maxlen=max_sequence_len)\n",
        "x_valid_padded = pad_sequences(x_valid_seq, padding='post', maxlen=max_sequence_len)\n",
        "x_test_padded = pad_sequences(x_test_seq, padding='post', maxlen=max_sequence_len)\n",
        "\n",
        "#print(x_train_padded[0])\n",
        "#print(x_train_padded.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6X3LvAA4tf-"
      },
      "source": [
        "# 4\n",
        "\n",
        "Next, we set out building our recurrent neural network (RNN). The \"recurrent\" in our RNN means the output at the current time step becomes the input to the next time step. So at each point in the sequence, the model not only considers the current input, but also what it remembers about preceding elements(Koehrsen, 2018).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC_HWKlIi2Wn"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in4zjfuAi2vX"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Convolution1D\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "\n",
        "embed_size = 128\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_vocab_size, embed_size))\n",
        "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dropout(0.05))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNqI_Qd3CtzL",
        "outputId": "368d8989-eb71-4955-87b9-03ccae688a6b"
      },
      "source": [
        "type(y_train[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2dYBQmoBURj",
        "outputId": "b51a4956-d25d-42e4-e9e0-e4e8159db40b"
      },
      "source": [
        "batch_size = 100\n",
        "epochs = 6\n",
        "model.fit(x_train_padded, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "80/80 [==============================] - 3s 36ms/step - loss: 0.5951 - accuracy: 0.6858 - val_loss: 0.3589 - val_accuracy: 0.8585\n",
            "Epoch 2/6\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.2927 - accuracy: 0.8900 - val_loss: 0.3144 - val_accuracy: 0.8680\n",
            "Epoch 3/6\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.1694 - accuracy: 0.9424 - val_loss: 0.3276 - val_accuracy: 0.8635\n",
            "Epoch 4/6\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.1120 - accuracy: 0.9636 - val_loss: 0.3984 - val_accuracy: 0.8630\n",
            "Epoch 5/6\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.0703 - accuracy: 0.9786 - val_loss: 0.4350 - val_accuracy: 0.8535\n",
            "Epoch 6/6\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.0435 - accuracy: 0.9877 - val_loss: 0.6145 - val_accuracy: 0.8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff1b430f908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8DDt1fri3M_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0UUYFBtA9pI"
      },
      "source": [
        "# 5 \n",
        "# Evaluate Final Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAnXuuKJBDIB",
        "outputId": "ba281bc8-92c4-41fe-c4e0-e0f1d7ddd4e0"
      },
      "source": [
        "print(\"Evaluate the test data set.\")\n",
        "results = model.evaluate(x_test_padded, y_test, batch_size=batch_size )\n",
        "print(f'test loss (binary_crossentropy): {results[0]}')\n",
        "print(f'test accuracy:                   {results[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate the test data set.\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.6641 - accuracy: 0.8400\n",
            "test loss (binary_crossentropy): 0.6640698909759521\n",
            "test accuracy:                   0.8399999737739563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW8vu59ZjPGH"
      },
      "source": [
        "# 6\n",
        "# Summary and Findings\n",
        "## Text cleaning\n",
        "  There was a great deal of text cleaning that had to be done before any text sentiment models to be built. For instance everything was made lowercase, number, punctuation, and extra white space removed, etc. We also removed stopwords from the reviews. Stopwords are ones like 'in', 'this', 'below', 'you', 'between', 'above', etc.--humans use them to communicate, but they're not useful to the computer. Last we used the WordNetLemmatizer to convert all words to their root words.\n",
        "\n",
        "## Our basic rule-based sentiment model\n",
        "  For our rule-based sentiment model, the AI determined a review's sentiment score by subtracting the number of negative words in a sequence from the number of positive in the same sequence. The model is simple, but not perfect- for instance, it has shown it's unreliable with reading over double-negatives. Still the simple model had 73% accuracy with the test data.\n",
        "\n",
        "## RNN\n",
        "\n",
        "  For our models we capped the 'max review length' at 130 words. We fiddled with the max_vocab_size, and settled at 6000. We started with 10 epochs, but determined we didn't need more than 6.\n",
        "  Our final model had 83.99% accuracy on the test set, significantly more accurate than our simple rule-based sentiment model.\n",
        "\n",
        "## *Models*\n",
        "\n",
        "#######################\n",
        "In our two models we create, we find that...\n",
        "\n",
        "Examples of things to talk about:\n",
        "\n",
        "- We tried a few cleaning processes. In fact we decided that we should treat apostrophes (') different than spaces. This is because...\n",
        "\n",
        "- We found that after several tries, the RNN did finally converge to (insert metrics).\n",
        "\n",
        "- We find that all the processing and RNN training only gives us a ??%-point gain over our simple rule based benchmark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oagwpikXiDv6"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHqG1jj29Fmy"
      },
      "source": [
        "Works Cited\n",
        "\n",
        "Koehrsen, Will. (Nov. 4, 2018). \"Recurrent Neural Networks by Example in Python\" towardsdatascience.com. Retrieved from:\n",
        "https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470\n",
        "\n",
        "Many models adapted from 'L10_Lab_NLP' notebook.\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    }
  ]
}